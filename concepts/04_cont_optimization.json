{
  "name" : "Continuous Optimization",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Many learning algorithms involve search for parameters that maximize a _learning objective_ (or minimize an empirical _learning loss_). In most cases this means optimizing a real-valued function over a continuous search or parameter space. Wolfe currently solves such optimization problems by [automatic differentiation](todo) of the objective term in combination with gradient-based numerical optimization algorithms. \n\n### Automatic Differentiation\n\nWhen you define a term with free continuous, vector or matrix variables you can ask Wolfe to calculate the gradient with respect to these variables. This is achieved through forward propagation (evaluation) followed by [backpropagation](todo) of the error signal. Usually you don't need these gradients directly, they are primarily used within optimization algorithms. However, explicitly calculating the gradient can still be useful, for example for debugging or when one develops extensions ([new terms](todo)) for Wolfe. \n\nTo get the gradient of term `t` with respect to the free variable `x` use `t.diff(x1)(x1 := v1, x2 := v2, ...)`. \n\n",
      "extraFields" : { },
      "outputFormat" : "<p>Many learning algorithms involve search for parameters that maximize a <em>learning objective</em> (or minimize an empirical <em>learning loss</em>). In most cases this means optimizing a real-valued function over a continuous search or parameter space. Wolfe currently solves such optimization problems by <a href=\"todo\">automatic differentiation</a> of the objective term in combination with gradient-based numerical optimization algorithms. </p><h3>Automatic Differentiation</h3><p>When you define a term with free continuous, vector or matrix variables you can ask Wolfe to calculate the gradient with respect to these variables. This is achieved through forward propagation (evaluation) followed by <a href=\"todo\">backpropagation</a> of the error signal. Usually you don't need these gradients directly, they are primarily used within optimization algorithms. However, explicitly calculating the gradient can still be useful, for example for debugging or when one develops extensions (<a href=\"todo\">new terms</a>) for Wolfe. </p><p>To get the gradient of term <code>t</code> with respect to the free variable <code>x</code> use <code>t.diff(x1)(x1 := v1, x2 := v2, ...)</code>. </p>"
    }
  }, {
    "id" : 1,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val x = Doubles.Var\nval b = Bools.Var\nval t = x * 4.0 - x * x * I(b)\nt.diff(x)(x := 1.0, b := true)",
      "extraFields" : {
        "aggregatedCells" : "[]"
      },
      "outputFormat" : "<div class=\"string-result\"><span class=\"asString String\">2.0</span></div>"
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Gradient Based Optimization\n\nOnce a objective over a continuous domain is defined, it can be optimized using the `argmax(dom)(obj)` operator, where `dom` is a continuous domain and `obj` is a function of type `dom.Term => DoubleTerm`.  ",
      "extraFields" : { },
      "outputFormat" : "<h3>Gradient Based Optimization</h3><p>Once a objective over a continuous domain is defined, it can be optimized using the <code>argmax(dom)(obj)</code> operator, where <code>dom</code> is a continuous domain and <code>obj</code> is a function of type <code>dom.Term =&gt; DoubleTerm</code>. </p>"
    }
  }, {
    "id" : 3,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def obj(x:DoubleTerm) = x * 4.0 - x * x \nval term = argmax(Doubles)(obj) by adaGrad(AdaGradParameters(50,1))\nterm.eval()",
      "extraFields" : {
        "aggregatedCells" : "[\"val x = Doubles.Var\\nval b = Bools.Var\\nval t = x * 4.0 - x * x * I(b)\\nt.diff(x)(x := 1.0, b := true)\"]"
      },
      "outputFormat" : "<div class=\"string-result\"><span class=\"asString String\">1.9999999999988005</span></div>"
    }
  }, {
    "id" : 4,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Notice that Wolfe supports stochastic optimization as well, but this requires the use of [stochastic terms](todo). \n\n### Composed Parameter Spaces\nIn some cases the your parameter space can be captured with a single vector, or maybe matrix. In others it may be composed of vectors of different roles and dimensions. For example, a document classifier may use as parameters a weight vector for traditional features and one low-dimensional embedding per word.  ",
      "extraFields" : { },
      "outputFormat" : "<p>Notice that Wolfe supports stochastic optimization as well, but this requires the use of <a href=\"todo\">stochastic terms</a>. </p><h3>Composed Parameter Spaces</h3><p>In some cases the your parameter space can be captured with a single vector, or maybe matrix. In others it may be composed of vectors of different roles and dimensions. For example, a document classifier may use as parameters a weight vector for traditional features and one low-dimensional embedding per word. </p>"
    }
  }, {
    "id" : 5,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "@domain case class Theta(x:Vect,y:IndexedSeq[Double])\nval Thetas = Theta.Values(Vectors(2),Seqs(Doubles,3))\ndef obj(t:Thetas.Term) = \n  sum(0 until 3) { i => 4.0 * t.y(i) - t.y(i) * t.y(i)} + (t.x dot t.x) \nval term = argmax(Thetas)(obj) by adaGrad(AdaGradParameters(50,1))\nterm.eval()\n",
      "extraFields" : {
        "aggregatedCells" : "[\"val x = Doubles.Var\\nval b = Bools.Var\\nval t = x * 4.0 - x * x * I(b)\\nt.diff(x)(x := 1.0, b := true)\",\"def obj(x:DoubleTerm) = x * 4.0 - x * x \\nval term = argmax(Doubles)(obj) by adaGrad(AdaGradParameters(50,1))\\nterm.eval()\"]"
      },
      "outputFormat" : "<div class=\"string-result\"><div class=\"asProduct Theta\"><span class=\"typeName\">Theta</span>\n<ul class=\"fields\">\n  <li class=\"field\"><span class=\"fieldName\">x</span> <span class=\"fieldValue\"><span class=\"asString String\">Tensor1(0.0,0.0)</span></span></li>\n  <li class=\"field\"><span class=\"fieldName\">y</span> <span class=\"fieldValue\"><div class=\"asIterable Vector\"><span class=\"typeName\">Vector</span>\n    <ol start=\"0\" class=\"fields\">\n      <li class=\"fieldValue\"><span class=\"asString String\">1.9999999999988005</span></li>\n      <li class=\"fieldValue\"><span class=\"asString String\">1.9999999999988005</span></li>\n      <li class=\"fieldValue\"><span class=\"asString String\">1.9999999999988005</span></li>\n    </ol>\n</div></span></li>\n</ul>\n</div></div>"
    }
  }, {
    "id" : 6,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Nested Discrete Optimization\nOne of wolfe's most powerful features is the ability to nest discrete maximization within continuous objectives. This enables users to develop structured prediction models where the per-instance loss usually involves  discrete maximization over a structured search space, such as a sequence of tags.     \n\n(Note that the code below works in offline wolfe, but currently not in moro)",
      "extraFields" : { },
      "outputFormat" : "<h3>Nested Discrete Optimization</h3><p>One of wolfe's most powerful features is the ability to nest discrete maximization within continuous objectives. This enables users to develop structured prediction models where the per-instance loss usually involves discrete maximization over a structured search space, such as a sequence of tags. </p><p>(Note that the code below works in offline wolfe, but currently not in moro)</p>"
    }
  }, {
    "id" : 7,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "implicit val random = new scala.util.Random(0)\n\nval mpParams = BPParameters(10)\nval adaParams = AdaGradParameters(100, 0.1)\n\nval n = 5\nval X = Seqs(Ints(0 until 3), 0, n)\nval Y = Seqs(Bools, 0, n)\nimplicit val Weights = Vectors(5)\n\n@domain case class Instance(x: X.Value, y: Y.Value)\n\nimplicit val Instances = Instance.Values(X, Y)\n\n// val train = Seq(Instance(IndexedSeq(0, 1, 2), IndexedSeq(true, false, true))).toConst\n\n// def model(x: X.Term, w: Weights.Term)(y: Y.Term) =\n// sum(0 until x.length) { i => w dot oneHot(x(i), I(y(i)))}\n\n// def hammingDistance(y1: Y.Term, y2:Y.Term) =\n// sum(0 until y1.length) { j => I(y1(j) <-> ! y2(j)) }\n\n// def instanceScore(x: X.Term, yGold: Y.Term)(w: Weights.Term) = {\n// model(x, w)(yGold) -\n//   { max(Y)(y => model(x, w)(y) + hammingDistance(yGold, y)) by maxProduct(mpParams) }\n// }\n\n// def score(w: Weights.Term) = sum(train) { i => instanceScore(i.x, i.y)(w)} argmaxBy adaGrad(adaParams)\n\n// def stochasticScore(w: Weights.Term) = shuffled(train) { i => instanceScore(i.x, i.y)(w)} argmaxBy adaGrad(adaParams)\n\n// val wStar = argmax(Weights)(stochasticScore)\n\n// wStar.eval()",
      "extraFields" : {
        "aggregatedCells" : "[\"val x = Doubles.Var\\nval b = Bools.Var\\nval t = x * 4.0 - x * x * I(b)\\nt.diff(x)(x := 1.0, b := true)\",\"def obj(x:DoubleTerm) = x * 4.0 - x * x \\nval term = argmax(Doubles)(obj) by adaGrad(AdaGradParameters(50,1))\\nterm.eval()\",\"@domain case class Theta(x:Vect,y:IndexedSeq[Double])\\nval Thetas = Theta.Values(Vectors(2),Seqs(Doubles,3))\\ndef obj(t:Thetas.Term) = \\n  sum(0 until 3) { i => 4.0 * t.y(i) - t.y(i) * t.y(i)} + (t.x dot t.x) \\nval term = argmax(Thetas)(obj) by adaGrad(AdaGradParameters(50,1))\\nterm.eval()\\n\"]"
      },
      "outputFormat" : "<div class=\"string-result\"><span class=\"label label-danger\">Error!</span>\n<pre class=\"error\">java.lang.AbstractMethodError\n  at ml.wolfe.term.Dom$DomTerm$class.$init$(Dom.scala:49)\n  ... 77 elided\n</pre></div>"
    }
  } ],
  "config" : { }
}
