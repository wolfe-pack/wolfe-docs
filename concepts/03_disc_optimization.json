{
  "name" : "Discrete Optimization",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Discrete Optimization is a corner stone of [structured prediction](http://en.wikipedia.org/wiki/Structured_prediction). Both at _test time_ when you need to find the structure (say, sequence, parse tree, or general graph) with maximum score or probability, and at _training time_ when you try to ensure that the maximizing structure is equal to the _gold_ structure in the training set.\n\n ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 1,
    "compiler" : "heading3",
    "input" : {
      "sessionId" : null,
      "code" : "Exhaustive Search",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Let us first consider the simplest case of structured prediction: binary classification. Given a linear `model`, learned `weights` and an `observed` feature vector, we like to find the boolean label `y` that maximizes the score. This can be done using the `argmax(domain)(obj)` expression. Here `domain` is a domain object, and `obj` a function from `domain.Term` to `Doubles`. By default `argmax` performs an exhaustive search through the set of all objects in `domain`. Notice that `argmax` itself is a term, and hence needs a `()` to be evaluated.  ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 3,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val W = Vectors(2)\nval weights = W.Const(vector(1,2))\nval observed = W.Const(vector(1,-1))\nargmax(Bools)(y =>  I(y) * (weights dot observed))()",
      "extraFields" : {
        "aggregatedCells" : "[]"
      },
      "outputFormat" : null
    }
  }, {
    "id" : 4,
    "compiler" : "heading3",
    "input" : {
      "sessionId" : null,
      "code" : "Message Passing",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "The above binary search space is obviously small enough to search brute-force. However, in general search spaces have combinatorial size, for example, when we are searching for sequences of given length. In such cases exhaustive search becomes infeasible. For such cases Wolfe offers combinatorial optimization algorithms based on message passing. To employ these methods, Wolfe analyses the term structure to create factor graphs, and then runs message passing on these graphs. \n\nTo use message passing when finding argmax values you can use the `argmax(dom)(obj) by method` expression. Currently Wolfe only supports the [Max-Product](todo) algorithm, a generalization of the [Viterbi](todo) algorithm. Below you find an example where a [linear chain CRF](todo) is optimized using Max-Product.            ",
      "extraFields" : { },
      "outputFormat" : null
    }
  }, {
    "id" : 6,
    "compiler" : "wolfe",
    "input" : {
      "sessionId" : null,
      "code" : "val l = Ints(0 until 10).Var\nval Y = Seqs(Bools, 0, 5)\ndef model(y: Y.Term) = { \n  sum(0 until l) { i => I(y(i))} +\n  sum(0 until l - 1) { i => I(y(i) <-> !y(i + 1))}\n} subjectTo (y.length === l) \nval result = argmax(Y)(model) by maxProduct(MaxProductParameters(10))\nresult(l := 5) ",
      "extraFields" : {
        "aggregatedCells" : "[\"val W = Vectors(2)\\nval weights = W.Const(vector(1,2))\\nval observed = W.Const(vector(1,-1))\\nargmax(Bools)(y =>  I(y) * (weights dot observed))()\"]"
      },
      "outputFormat" : null
    }
  } ],
  "config" : { }
}
