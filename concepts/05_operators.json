{
  "name" : "Operators",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "code" : "The previous sections have shown how Scala and Wolfe can be used\nto develop the building blocks of machine learning models. \nTo do inference and learning with such models we require mathematical operators\nthat interfact with the building blocks we introduced earlier. \n\nMost wolfe operators are used as in\n$$\n\\DeclareMathOperator{\\operator}{operator}\n\\operator\\_{s \\in \\mathcal{S}: c(s) } f(s)\n$$\nwhere \\\\(\\mathcal{S}\\\\) is a collection of possible worlds, \\\\(c(\\cdot)\\\\)\na constraint on possible worlds, and \\\\(f\\\\) a scalar function. In Scala\nthe operator will be a function with two (curried) arguments, one for the domain\n(including a potential filter) and one for the function. \n\nTo make this more concrete, let's look at the (brute-force) definition of the \nargmax operator. Note that `maxBy` is a Scala collections function on iterables.  \n\n",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 1,
    "compiler" : "wolfe",
    "input" : {
      "code" : "def bruteArgmax[T](S:Iterable[T])(f:T=>Double) = S.maxBy(f)\nbruteArgmax((0 until 5) filter (_ < 3)) { s => s.toDouble }",
      "outputFormat" : "string",
      "extraFields" : null
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "code" : "All Wolfe operators have simple brute-force implementations like the one above. The \ncompiler (in the form of our Wolfe macros) attempts to replace these brute-force\nimplementations with optimized one, but may back off to brute-force if no optimization can be\ndone.",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 3,
    "compiler" : "heading3",
    "input" : {
      "code" : "Restrictions",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 4,
    "compiler" : "markdown",
    "input" : {
      "code" : "When models interact with operators the __restrictions of Wolfe__ with respect\nto possible worlds, constraints and functions will become apparent. While you can \nuse any Scala expression to define these building blocks, only for a subset \nof these Wolfe operators can be implemented efficiently. This is partly \nbecause the internal matching algorithms lack coverage, and partly because\nthey are certainly models for which there simply is not effective algorithm, even\nif they can be matched. At least for the coverage problem we hope to continuously\nimprove Wolfe. Moreover, Wolfe attempts to yield errors and warnings whenever building blocks are used\nthat cannot be optimized.  \n\nOne restriction is that collections of possible worlds, if not atomic, need to be defined\nusing `def` and not `val` (or used inline directly when the operator is called).\nIf `val` is used the collection effectively becomes atomic, and a lot of optimizations\ncannot be performed. You may test this in the examples below\nby replacing `def` with `val` definitions.\n\n\n",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 5,
    "compiler" : "heading3",
    "input" : {
      "code" : "Argmax",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 6,
    "compiler" : "markdown",
    "input" : {
      "code" : "Finding the argument that maximizes a function under some constraint is a fundamental \nproblem of machine learning and beyond. Wolfe currently performs two types of optimizations\ndepending on the type of search space.\n\n",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 7,
    "compiler" : "heading4",
    "input" : {
      "code" : "Discrete",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 8,
    "compiler" : "markdown",
    "input" : {
      "code" : "Discrete optimization can be used for *maximum-a-priori* (MAP) inference: finding the most likely\nor highest scoring structure given some observation and a model. \n\nIn Wolfe discrete optimization is performed whenever the search space is not a vector space.\nIn particular, even if the search space has continuous components discrete optimization is performed\nas long as the continuous components are observed. \n\nInternally Wolfe performs discrete optimization (and other operations on discrete\nspaces) by first creating a [factor graph](http://en.wikipedia.org/wiki/Factor_graph)\nrepresentation, and then running MAP inference code on this graph. In some sense\nthis process is an implementation detail that should be hidden from the user,\nin particular because it may change over time. However, without understanding\nthis process users may not understand why some models are slow and others aren't.\nWe hence aim at describing the process in more detail in the future.\n\nThe example below shows a simple application of the `argmax` operator. The operator\ntakes as first argument a search space, in this case `worlds where (_.smokesAnn)`. \nThis space is already constrained using the filter `_.smokesAnn` which is a \nScala short form of `w => w.smokesAnn`. Wolfe analyzes the syntax tree of the\n`worlds where (_.smokesAnn)` expression, translates it into a factor graph with two\nboolean variables (corresponding to `smokesAnn` and `smokesBob`), and sets the variable for\n`smokesAnn` to `true` based on the constraint `_.smokesAnn`.  \n",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 9,
    "compiler" : "wolfe",
    "input" : {
      "code" : "case class World(smokesAnn:Boolean, smokesBob:Boolean)\ndef worlds = all(World) {bools x bools}\nargmax(worlds where (_.smokesAnn)) {w => I(w.smokesAnn == w.smokesBob)}",
      "outputFormat" : "string",
      "extraFields" : null
    }
  }, {
    "id" : 10,
    "compiler" : "heading3",
    "input" : {
      "code" : "Vector-Valued",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 11,
    "compiler" : "markdown",
    "input" : {
      "code" : "Along with objectives over discrete spaces, Wolfe can optimize over \n[vector-spaces](http://moro.wolfe.ml:9000/doc/wolfe-static/wolfe/docs/concepts/03_vectors) too,\nprovided it can calculate the gradient of the function,\nand no constraints or filters are applied to the vector space. \nTo implement this type optimization Wolfe calculates the symbolic\n(sub)gradient of the objective, and then uses this gradient \nin gradient-based optimization through the [FACTORIE](http://factorie.cs.umass.edu/) optimization\npackage. \n\nBelow you see a typical objective, stemming from a perceptron style loss (line 6)\nthat attempts to get the current most likely solution (result of the max\nexpression) close to the score `s(w)(i)` of the training instance `i`.  \n\n\n",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 12,
    "compiler" : "wolfe",
    "input" : {
      "code" : "case class XY(x:Int,y:Int)\ndef space = all(XY) { (0 until 2) x (0 until 2) }\ndef f(xy:XY) = oneHot(xy.x -> xy.y)\ndef s(w:Vector)(xy:XY) = w dot f(xy)\ndef loss(data:Seq[XY])(w:Vector) = \n  sum(data) {i => s(w)(i) - max(space where (_.x == i.x)) { s(w) } }\nval training = for (i <- 0 until 2) yield XY(i,i)\nval w = argmax(vectors) { loss(training) }  \nw",
      "outputFormat" : "string",
      "extraFields" : null
    }
  }, {
    "id" : 13,
    "compiler" : "markdown",
    "input" : {
      "code" : "Wolfe also provides an `argmin` operator returns the minimizing argument of a function. ",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 14,
    "compiler" : "heading3",
    "input" : {
      "code" : "Mapper",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 15,
    "compiler" : "markdown",
    "input" : {
      "code" : "Often compiled operators could share resources between calls. For example, \nsay you have a prediction model and trained its weight one some data before. Now you\nwant to apply this model to sequence of test instances. This would \nrequire an argmax operation for each training instance, and these operations\neach require almost the same internal vector-indices. Wolfe can \nperform sharing between such operations by using the `map` operator which\nalmost works like a normal `map` call on a collection but can share resources\nbetween instances.\n\n\n",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 16,
    "compiler" : "wolfe",
    "input" : {
      "code" : "def h(w:Vector)(xy:XY) = argmax(space where (_.x == xy.x)) { s(w) }\nval test = Seq(XY(0,0),XY(1,1))\nval t1 = test.map(h(w))\nval t2 = map(test){h(w)}\nt1 == t2",
      "outputFormat" : "string",
      "extraFields" : null
    }
  }, {
    "id" : 17,
    "compiler" : "heading3",
    "input" : {
      "code" : "Operator Hints",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 18,
    "compiler" : "markdown",
    "input" : {
      "code" : "When executing an operation such as `argmax` Wolfe has a wide range of algorithms\nand parameters to choose. While Wolfe tries to automatically \ndecide what's best, often the user's background knowledge is difficult to beat. \n\nWolfe allows users to provide hints to operators that indicate which algorithm\nor configuration should be used. This can be done by using a scala annotation\non the objective the operator is applied to. For example, in the snippet\nbelow we tell Wolfe to run [Max-Product Belief Propagation](http://en.wikipedia.org/wiki/Belief_propagation) \nwith 10 iterations in order to optimize `f`. \n\n",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 19,
    "compiler" : "wolfe",
    "input" : {
      "code" : "@OptimizeByInference(BeliefPropagation.maxProduct(5))\ndef annotated(w:World) = I(w.smokesAnn == w.smokesBob) \nargmax(worlds where (_.smokesAnn)) { annotated }",
      "outputFormat" : "string",
      "extraFields" : null
    }
  }, {
    "id" : 20,
    "compiler" : "markdown",
    "input" : {
      "code" : "The following example shows how we choose a different FACTORIE \ngradient-based optimizer to optimize the training loss. ",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 21,
    "compiler" : "wolfe",
    "input" : {
      "code" : "import cc.factorie.optimize.{Perceptron}\n@OptimizeByLearning(Learn.online(3, new Perceptron))\ndef annotatedLoss(data:Seq[XY])(w:Vector) = loss(data)(w)\nargmax(vectors) { annotatedLoss(training) }",
      "outputFormat" : "string",
      "extraFields" : null
    }
  } ]
}
