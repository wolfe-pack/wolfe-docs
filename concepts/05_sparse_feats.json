{
  "name" : "Sparse Features",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "In many machine learning applications, in particular within the area of Natural Language Processing, sparse feature representations of inputs and outputs are important. For example, in many NLP applications the word \\\\(w_i\\\\) at a given token \\\\(i\\\\) is represented as one-hot vector \\\\(\\mathbf{x} \\in \\mathbb{R}^{|V|}\\\\) where \\\\(V\\\\) is the vocabularly, and the vector is 'hot' (non-zero) at exactly one position corresponding to identity of the word \\\\(w_i\\\\). We may not only care about the current word, but also about the previous and next one, and in this case we like to use a vector in \\\\(\\mathbb{R}^{3|V|}\\\\) with 3 hot positions, one for the previous, one for the current and one for next word. In structured prediction these sparse representations are further conjoined with the current and previous label we like to predict. And even when the model uses dense and neural representations, at the input layers these approaches often require sparse representations.\n\n### One-Hot Vector Terms\nWolfe allows users to compose arbitrary complex sparse feature representations. The main building block are terms that represent simple one-hot vectors. Crucially, the hot components of the vector can be defined through arbitrary values, not just integer indices. This makes it easy to produce oneHot vectors for words, pairs of words, tuples of labels, current words, previous words etc. However, internally oneHot vectors are still simple integer-indexed vectors (designed to be fast). To bridge this mismatch from user-defined value-indexed vectors to fast integer indexed vectors, wolfe provides _feature indices_: mappings from tuples of values to integer indices. Once such an index is created, one-hot vectors can be constructed using the index.    ",
      "extraFields" : { },
      "outputFormat" : "<p>In many machine learning applications, in particular within the area of Natural Language Processing, sparse feature representations of inputs and outputs are important. For example, in many NLP applications the word <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-1\" role=\"math\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.015em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.61em 1000.003em 2.562em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"msubsup\" id=\"MathJax-Span-3\"><span style=\"display: inline-block; position: relative; width: 0.955em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.658em;\"><span class=\"mi\" id=\"MathJax-Span-5\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.861em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-1\">w_i</script> at a given token <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-6\" role=\"math\" style=\"width: 0.36em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.301em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.729em 1000.003em 2.741em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-7\"><span class=\"mi\" id=\"MathJax-Span-8\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.932em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-2\">i</script> is represented as one-hot vector <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-9\" role=\"math\" style=\"width: 4.11em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.396em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.491em 1000.003em 2.741em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-10\"><span class=\"texatom\" id=\"MathJax-Span-11\"><span class=\"mrow\" id=\"MathJax-Span-12\"><span class=\"mi\" id=\"MathJax-Span-13\" style=\"font-family: STIXGeneral; font-weight: bold;\">x</span></span></span><span class=\"mo\" id=\"MathJax-Span-14\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.301em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-15\" style=\"padding-left: 0.301em;\"><span style=\"display: inline-block; position: relative; width: 1.61em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-16\"><span class=\"mrow\" id=\"MathJax-Span-17\"><span class=\"mi\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 0.777em;\"><span class=\"texatom\" id=\"MathJax-Span-19\"><span class=\"mrow\" id=\"MathJax-Span-20\"><span class=\"texatom\" id=\"MathJax-Span-21\"><span class=\"mrow\" id=\"MathJax-Span-22\"><span class=\"mo\" id=\"MathJax-Span-23\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-24\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-25\"><span class=\"mrow\" id=\"MathJax-Span-26\"><span class=\"mo\" id=\"MathJax-Span-27\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">|</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.218em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-3\">\\mathbf{x} \\in \\mathbb{R}^{|V|}</script> where <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-28\" role=\"math\" style=\"width: 0.896em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.717em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.729em 1000.003em 2.741em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-29\"><span class=\"mi\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.932em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-4\">V</script> is the vocabularly, and the vector is 'hot' (non-zero) at exactly one position corresponding to identity of the word <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-31\" role=\"math\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.015em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.61em 1000.003em 2.562em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-32\"><span class=\"msubsup\" id=\"MathJax-Span-33\"><span style=\"display: inline-block; position: relative; width: 0.955em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-34\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.658em;\"><span class=\"mi\" id=\"MathJax-Span-35\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.861em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-5\">w_i</script>. We may not only care about the current word, but also about the previous and next one, and in this case we like to use a vector in <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-36\" role=\"math\" style=\"width: 2.443em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.027em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em 1000.003em 2.384em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-37\"><span class=\"msubsup\" id=\"MathJax-Span-38\"><span style=\"display: inline-block; position: relative; width: 1.967em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-39\"><span class=\"mrow\" id=\"MathJax-Span-40\"><span class=\"mi\" id=\"MathJax-Span-41\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 0.777em;\"><span class=\"texatom\" id=\"MathJax-Span-42\"><span class=\"mrow\" id=\"MathJax-Span-43\"><span class=\"mn\" id=\"MathJax-Span-44\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span><span class=\"texatom\" id=\"MathJax-Span-45\"><span class=\"mrow\" id=\"MathJax-Span-46\"><span class=\"mo\" id=\"MathJax-Span-47\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-48\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-49\"><span class=\"mrow\" id=\"MathJax-Span-50\"><span class=\"mo\" id=\"MathJax-Span-51\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">|</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.218em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-6\">\\mathbb{R}^{3|V|}</script> with 3 hot positions, one for the previous, one for the current and one for next word. In structured prediction these sparse representations are further conjoined with the current and previous label we like to predict. And even when the model uses dense and neural representations, at the input layers these approaches often require sparse representations.</p><h3>One-Hot Vector Terms</h3><p>Wolfe allows users to compose arbitrary complex sparse feature representations. The main building block are terms that represent simple one-hot vectors. Crucially, the hot components of the vector can be defined through arbitrary values, not just integer indices. This makes it easy to produce oneHot vectors for words, pairs of words, tuples of labels, current words, previous words etc. However, internally oneHot vectors are still simple integer-indexed vectors (designed to be fast). To bridge this mismatch from user-defined value-indexed vectors to fast integer indexed vectors, wolfe provides <em>feature indices</em>: mappings from tuples of values to integer indices. Once such an index is created, one-hot vectors can be constructed using the index. </p>"
    }
  }, {
    "id" : 1,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val Words = Seq(\"the\", \"cat\", \"sat\").toDom\nval Vects = Vectors(Words.domainSize)\n\nval index = new SimpleFeatureIndex(Vects) //create index for the given domain\nval w = Words.Var\nval t = index.oneHot('f1, 2.1, w) //'f1 is the feature template name\nt",
      "extraFields" : {
        "aggregatedCells" : "[]"
      },
      "outputFormat" : "<div class=\"string-result\"><div id=\"term658902f2-282f-425c-8ad4-fde537cf63da\" class=\"term\">\n<svg width=\"960\" height=\"210\"><g transform=\"translate(120,40)\"><path class=\"link\" d=\"M75,0C75,25 37.5,25 37.5,50\"></path><path class=\"link\" d=\"M75,0C75,25 112.5,25 112.5,50\"></path><g class=\"node\" transform=\"translate(112.5,50)\"><circle r=\"10\"></circle><text y=\"18\" dy=\".35em\" text-anchor=\"middle\" style=\"fill-opacity: 1;\">2.1</text></g><g class=\"node\" transform=\"translate(37.5,50)\"><circle r=\"10\"></circle><text y=\"18\" dy=\".35em\" text-anchor=\"middle\" style=\"fill-opacity: 1;\">w </text></g><g class=\"node\" transform=\"translate(75,0)\"><circle r=\"10\"></circle><text y=\"-18\" dy=\".35em\" text-anchor=\"middle\" style=\"fill-opacity: 1;\">OneHot</text></g></g></svg>\n</div>\n<script>\n\nvar treeData = [\n\n{\n  \"name\": \"OneHot\",\n  \"parent\": \"null\",\n  \"children\": [\n{\n  \"name\": \"w \",\n  \"parent\": \"OneHot\",\n  \"children\": []\n}\n             ,\n\n{\n  \"name\": \"2.1\",\n  \"parent\": \"OneHot\",\n  \"children\": []\n}\n             ]\n}\n\n];\n\nvar depth = 2 + 1\n\n// ************** Generate the tree diagram\t *****************\nvar margin = {top: 40, right: 120, bottom: 20, left: 120},\n\twidth = 960 - margin.right - margin.left,\n\theight = depth * 70 - margin.top - margin.bottom;\n\nvar i = 0;\n\nvar tree = d3.layout.tree()\n\t.size([height, width]);\n\nvar diagonal = d3.svg.diagonal()\n\t.projection(function(d) { return [d.x, d.y]; });\n\nvar svg = d3.select(\"#term658902f2-282f-425c-8ad4-fde537cf63da svg\")\n\t.attr(\"width\", width + margin.right + margin.left)\n\t.attr(\"height\", height + margin.top + margin.bottom)\n  .append(\"g\")\n\t.attr(\"transform\", \"translate(\" + margin.left + \",\" + margin.top + \")\");\n\nroot = treeData[0];\n\nupdate(root);\n\nfunction update(source) {\n\n  // Compute the new tree layout.\n  var nodes = tree.nodes(source).reverse(),\n\t links = tree.links(nodes);\n\n  // Normalize for fixed-depth.\n  nodes.forEach(function(d) { d.y = d.depth * 50; });\n\n  // Declare the nodes…\n  var node = svg.selectAll(\"g.node\")\n\t  .data(nodes, function(d) { return d.id || (d.id = ++i); });\n\n  // Enter the nodes.\n  var nodeEnter = node.enter().append(\"g\")\n\t  .attr(\"class\", \"node\")\n\t  .attr(\"transform\", function(d) {\n\t\t  return \"translate(\" + d.x + \",\" + d.y + \")\"; });\n\n  nodeEnter.append(\"circle\")\n\t  .attr(\"r\", 10);\n\n  nodeEnter.append(\"text\")\n\t  .attr(\"y\", function(d) {\n\t\t  return d.children || d._children ? -18 : 18; })\n\t  .attr(\"dy\", \".35em\")\n\t  .attr(\"text-anchor\", \"middle\")\n\t  .text(function(d) { return d.name; })\n\t  .style(\"fill-opacity\", 1);\n\n  // Declare the links…\n  var link = svg.selectAll(\"path.link\")\n\t  .data(links, function(d) { return d.target.id; });\n\n  // Enter the links.\n  link.enter().insert(\"path\", \"g\")\n\t  .attr(\"class\", \"link\")\n\t  .attr(\"d\", diagonal);\n\n}\n\n</script></div>"
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "In the above example `t` is a term that represents a one-hot vector that has the value `2.1` at an index corresponding to the _feature template_ name `'f1` and the value of the word variable `w1`. For example, when the word variable has the value `\"the\"` the vector evaluates to:  ",
      "extraFields" : { },
      "outputFormat" : "<p>In the above example <code>t</code> is a term that represents a one-hot vector that has the value <code>2.1</code> at an index corresponding to the <em>feature template</em> name <code>'f1</code> and the value of the word variable <code>w1</code>. For example, when the word variable has the value <code>\"the\"</code> the vector evaluates to: </p>"
    }
  }, {
    "id" : 3,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val v = t.eval(w := \"the\")\nv",
      "extraFields" : {
        "aggregatedCells" : "[\"val Words = Seq(\\\"the\\\", \\\"cat\\\", \\\"sat\\\").toDom\\nval Vects = Vectors(Words.domainSize)\\n\\nval index = new SimpleFeatureIndex(Vects) //create index for the given domain\\nval w = Words.Var\\nval t = index.oneHot('f1, 2.1, w) //'f1 is the feature template name\\nt\"]"
      },
      "outputFormat" : "<div class=\"string-result\"><span class=\"asString String\">SparseIndexedTensor npos=1 sorted=0 ind=0,0,0,0 val=2.1,0.0,0.0,0.0</span></div>"
    }
  }, {
    "id" : 4,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "You can see a (Factorie) sparse vector which has the value `2.1` at index `0`, and the value zero everywhere else. To confirm this we can access it at any integer component:\n",
      "extraFields" : { },
      "outputFormat" : "<p>You can see a (Factorie) sparse vector which has the value <code>2.1</code> at index <code>0</code>, and the value zero everywhere else. To confirm this we can access it at any integer component:</p>"
    }
  }, {
    "id" : 5,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "v(0)",
      "extraFields" : {
        "aggregatedCells" : "[\"val Words = Seq(\\\"the\\\", \\\"cat\\\", \\\"sat\\\").toDom\\nval Vects = Vectors(Words.domainSize)\\n\\nval index = new SimpleFeatureIndex(Vects) //create index for the given domain\\nval w = Words.Var\\nval t = index.oneHot('f1, 2.1, w) //'f1 is the feature template name\\nt\",\"val v = t.eval(w := \\\"the\\\")\\nv\"]"
      },
      "outputFormat" : "<div class=\"string-result\"><span class=\"asString String\">2.1</span></div>"
    }
  }, {
    "id" : 6,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Depending on the value of the variable `w1` we can create different one-hot vectors:",
      "extraFields" : { },
      "outputFormat" : "<p>Depending on the value of the variable <code>w1</code> we can create different one-hot vectors:</p>"
    }
  }, {
    "id" : 7,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "t.eval(w := \"cat\")",
      "extraFields" : {
        "aggregatedCells" : "[\"val Words = Seq(\\\"the\\\", \\\"cat\\\", \\\"sat\\\").toDom\\nval Vects = Vectors(Words.domainSize)\\n\\nval index = new SimpleFeatureIndex(Vects) //create index for the given domain\\nval w = Words.Var\\nval t = index.oneHot('f1, 2.1, w) //'f1 is the feature template name\\nt\",\"val v = t.eval(w := \\\"the\\\")\\nv\",\"v(0)\"]"
      },
      "outputFormat" : "<div class=\"string-result\"><span class=\"asString String\">SparseIndexedTensor npos=1 sorted=0 ind=1,0,0,0 val=2.1,0.0,0.0,0.0</span></div>"
    }
  }, {
    "id" : 8,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "The internal vector representation these terms generate is detached from the original value-based indices we used to create the terms. To get a more interpretable representation, the index can be used to create a `Map` with key-value pairs for all non-zero components.",
      "extraFields" : {
        "aggregatedCells" : "[\"val Words = Seq(\\\"the\\\", \\\"cat\\\", \\\"sat\\\").toDom\\nval Vects = Vectors(100)\\n\\nval index = new SimpleFeatureIndex(Vects) //create index for the given domain\\nval w1 = Words.Var\\nval t = index.oneHot('f1, 2.1, w1)\\nt\",\"val v = t.eval(w1 := \\\"the\\\")\\nv\",\"v(0)\",\"t.eval(w1 := \\\"cat\\\")\"]"
      },
      "outputFormat" : "<p>The internal vector representation these terms generate is detached from the original value-based indices we used to create the terms. To get a more interpretable representation, the index can be used to create a <code>Map</code> with key-value pairs for all non-zero components.</p>"
    }
  }, {
    "id" : 9,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "index.toMap(t.eval(w := \"sat\"))",
      "extraFields" : {
        "aggregatedCells" : "[\"val Words = Seq(\\\"the\\\", \\\"cat\\\", \\\"sat\\\").toDom\\nval Vects = Vectors(Words.domainSize)\\n\\nval index = new SimpleFeatureIndex(Vects) //create index for the given domain\\nval w = Words.Var\\nval t = index.oneHot('f1, 2.1, w) //'f1 is the feature template name\\nt\",\"val v = t.eval(w := \\\"the\\\")\\nv\",\"v(0)\",\"t.eval(w := \\\"cat\\\")\"]"
      },
      "outputFormat" : "<div class=\"string-result\"><div class=\"asMap Map\"><span class=\"typeName\">Map</span>\n<ul class=\"fields\">\n  <li class=\"field\"><span class=\"fieldName\"><div class=\"asIterable Vector\"><span class=\"typeName\">Vector</span>\n    <ol start=\"0\" class=\"fields\">\n      <li class=\"fieldValue\"><span class=\"asString String\">'f1</span></li>\n      <li class=\"fieldValue\"><span class=\"asString String\">sat</span></li>\n    </ol>\n</div></span> <span class=\"fieldValue\"><span class=\"asString String\">2.1</span></span></li>\n</ul>\n</div></div>"
    }
  }, {
    "id" : 10,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Sparse Vector Terms\nOne-hot vector terms can be added to create sparse (\"multiple-hot\") vectors to create feature vectors that represent several aspects of the data at once. For example, to define a linear models over a word \\\\(w\\\\) and its part-of-speech tag \\\\(y\\\\) with weights \\\\(\\boldsymbol{\\theta}\\\\)\n$$\ns(w,y) = \\<\\boldsymbol{\\theta},\\boldsymbol{\\phi}(w,y)\\>\n$$\nwe may want to use the feature function \\\\(\\boldsymbol{\\phi}\\\\) to combine a bias feature with a feature to represent the conjunction of word and label.   ",
      "extraFields" : { },
      "outputFormat" : "<h3>Sparse Vector Terms</h3><p>One-hot vector terms can be added to create sparse (\"multiple-hot\") vectors to create feature vectors that represent several aspects of the data at once. For example, to define a linear models over a word <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-52\" role=\"math\" style=\"width: 0.896em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.717em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.967em 1000.003em 2.741em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-53\"><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Italic;\">w</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.718em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-7\">w</script> and its part-of-speech tag <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-55\" role=\"math\" style=\"width: 0.598em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.479em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.967em 1000.003em 2.92em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-56\"><span class=\"mi\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Italic;\">y</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.932em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-8\">y</script> with weights <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-58\" role=\"math\" style=\"width: 0.658em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.539em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.61em 1000.003em 2.682em -999.997em); top: -2.497em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-59\"><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.004em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-9\">\\boldsymbol{\\theta}</script> <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-61\" role=\"math\" style=\"width: 11.313em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.408em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.67em 1000.003em 2.92em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-62\"><span class=\"mi\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.182em;\">y</span><span class=\"mo\" id=\"MathJax-Span-68\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-69\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.301em;\">=<span style=\"font-family: STIXGeneral-Regular; font-style: normal; font-weight: normal;\">&lt;</span></span><span class=\"mi\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold; padding-left: 0.301em;\">θ</span><span class=\"mo\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold; padding-left: 0.182em;\">ϕ</span><span class=\"mo\" id=\"MathJax-Span-73\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-74\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-75\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-76\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.182em;\">y</span><span class=\"mo\" id=\"MathJax-Span-77\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-78\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.301em;\">&gt;</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.218em; vertical-align: -0.282em;\"></span></span></nobr></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-10\"> s(w,y) = <\\boldsymbol{\\theta},\\boldsymbol{\\phi}(w,y)> </script> we may want to use the feature function <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-79\" role=\"math\" style=\"width: 0.896em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.717em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.61em 1000.003em 2.86em -999.997em); top: -2.497em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-80\"><span class=\"mi\" id=\"MathJax-Span-81\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">ϕ</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.218em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-11\">\\boldsymbol{\\phi}</script> to combine a bias feature with a feature to represent the conjunction of word and label. </p>"
    }
  }, {
    "id" : 11,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val Tags = Seq('Det, 'Noun, 'Verb).toDom\nval Vects = Vectors(Words.domainSize * Tags.domainSize + 1) \nval index = new SimpleFeatureIndex(Vects)\ndef phi(w:Words.Term, y:Tags.Term) = {\n  import index._\n  oneHot('bias, y) + \n  oneHot('lex, y, w) \n}\nval y = Tags.Var \nval v = phi(w,y).eval(w := \"cat\", y := 'Noun)\nv",
      "extraFields" : {
        "aggregatedCells" : "[\"val Words = Seq(\\\"the\\\", \\\"cat\\\", \\\"sat\\\").toDom\\nval Vects = Vectors(Words.domainSize)\\n\\nval index = new SimpleFeatureIndex(Vects) //create index for the given domain\\nval w = Words.Var\\nval t = index.oneHot('f1, 2.1, w) //'f1 is the feature template name\\nt\",\"val v = t.eval(w := \\\"the\\\")\\nv\",\"v(0)\",\"t.eval(w := \\\"cat\\\")\",\"index.toMap(t.eval(w := \\\"sat\\\"))\"]"
      },
      "outputFormat" : "<div class=\"string-result\"><span class=\"asString String\">SparseIndexedTensor npos=2 sorted=0 ind=1,7,0,0 val=1.0,1.0,0.0,0.0</span></div>"
    }
  }, {
    "id" : 12,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Notice a few things: we use `import index._` to avoid prefixing `oneHot` with the index we use throughout the model (this is helpful when you use a lot of features); we don't provide a real feature value, in this case the default value is `1.0`. You can see that the result now has two hot components, at index `1` and `7`. This corresponds to two non-zero entries in the map view.",
      "extraFields" : { },
      "outputFormat" : "<p>Notice a few things: we use <code>import index._</code> to avoid prefixing <code>oneHot</code> with the index we use throughout the model (this is helpful when you use a lot of features); we don't provide a real feature value, in this case the default value is <code>1.0</code>. You can see that the result now has two hot components, at index <code>1</code> and <code>7</code>. This corresponds to two non-zero entries in the map view.</p>"
    }
  }, {
    "id" : 13,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "index.toMap(v)",
      "extraFields" : {
        "aggregatedCells" : "[\"val Words = Seq(\\\"the\\\", \\\"cat\\\", \\\"sat\\\").toDom\\nval Vects = Vectors(Words.domainSize)\\n\\nval index = new SimpleFeatureIndex(Vects) //create index for the given domain\\nval w = Words.Var\\nval t = index.oneHot('f1, 2.1, w) //'f1 is the feature template name\\nt\",\"val v = t.eval(w := \\\"the\\\")\\nv\",\"v(0)\",\"t.eval(w := \\\"cat\\\")\",\"index.toMap(t.eval(w := \\\"sat\\\"))\",\"val Tags = Seq('Det, 'Noun, 'Verb).toDom\\nval Vects = Vectors(Words.domainSize * Tags.domainSize + 1) \\nval index = new SimpleFeatureIndex(Vects)\\ndef phi(w:Words.Term, y:Tags.Term) = {\\n  import index._\\n  oneHot('bias, y) + \\n  oneHot('lex, y, w) \\n}\\nval y = Tags.Var \\nval v = phi(w,y).eval(w := \\\"cat\\\", y := 'Noun)\\nv\"]"
      },
      "outputFormat" : "<div class=\"string-result\"><div class=\"asMap Map\"><span class=\"typeName\">Map</span>\n<ul class=\"fields\">\n  <li class=\"field\"><span class=\"fieldName\"><div class=\"asIterable Vector\"><span class=\"typeName\">Vector</span>\n    <ol start=\"0\" class=\"fields\">\n      <li class=\"fieldValue\"><span class=\"asString String\">'bias</span></li>\n      <li class=\"fieldValue\"><span class=\"asString String\">'Noun</span></li>\n    </ol>\n</div></span> <span class=\"fieldValue\"><span class=\"asString String\">1.0</span></span></li>\n  <li class=\"field\"><span class=\"fieldName\"><div class=\"asIterable Vector\"><span class=\"typeName\">Vector</span>\n    <ol start=\"0\" class=\"fields\">\n      <li class=\"fieldValue\"><span class=\"asString String\">'lex</span></li>\n      <li class=\"fieldValue\"><span class=\"asString String\">'Noun</span></li>\n      <li class=\"fieldValue\"><span class=\"asString String\">cat</span></li>\n    </ol>\n</div></span> <span class=\"fieldValue\"><span class=\"asString String\">1.0</span></span></li>\n</ul>\n</div></div>"
    }
  }, {
    "id" : 14,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### Lazily Allocating Indices\nBy default value-based feature indices (such as the combination of word and tag in the example above) are mapped to integer indices _eagerly_, in the sense that we allocate one integer index for each possible combination of values. So in the above case, we indeed allocate `Words.domainSize * Tags.domainSize + 1` components of the vector space. This is generally a good idea, because this type of eager indices allows for fast, hashmap-free implementations of the mapping. However, when the domains of the index components (such as the vocabularly size) are getting too large, and we combine too many components (say, the current, the previous and the next word), a large number of indices need to be allocated and hence a large vector domain is needed. This in turn may lead to memory issues. \n\nIn wolfe this problem can be avoided by creating one-hot vectors that allocate their integer indices lazily, on demand. This is useful because in many cases (depending on the training algorithm used) it is very likely that only a small subset of feature indices will receive non-zero weights: those that are observed in the data, and those that are predicted by the model during the training epochs. For example, a one-hot feature that checks current, next and previous word, and conjoins it with the part-of-speech label, will have zero weights whenever the triple of words hasn't been observed in a consecutive order in the training set. To activate lazy allocation of indices, pass the option `eager = false` to the one-hot term.\n\nBelow we create a feature that conjoins two words. With eager allocation vectors have dimension `9` based on the `Words` domain defined above. With lazy allocation we can use, for example, vectors of dimension 2, provided that we only evaluate the term with two word pairs. Try changing `eager` to `true` and check whether the sparse vector has still dimension 2.",
      "extraFields" : { },
      "outputFormat" : "<h3>Lazily Allocating Indices</h3><p>By default value-based feature indices (such as the combination of word and tag in the example above) are mapped to integer indices <em>eagerly</em>, in the sense that we allocate one integer index for each possible combination of values. So in the above case, we indeed allocate <code>Words.domainSize * Tags.domainSize + 1</code> components of the vector space. This is generally a good idea, because this type of eager indices allows for fast, hashmap-free implementations of the mapping. However, when the domains of the index components (such as the vocabularly size) are getting too large, and we combine too many components (say, the current, the previous and the next word), a large number of indices need to be allocated and hence a large vector domain is needed. This in turn may lead to memory issues. </p><p>In wolfe this problem can be avoided by creating one-hot vectors that allocate their integer indices lazily, on demand. This is useful because in many cases (depending on the training algorithm used) it is very likely that only a small subset of feature indices will receive non-zero weights: those that are observed in the data, and those that are predicted by the model during the training epochs. For example, a one-hot feature that checks current, next and previous word, and conjoins it with the part-of-speech label, will have zero weights whenever the triple of words hasn't been observed in a consecutive order in the training set. To activate lazy allocation of indices, pass the option <code>eager = false</code> to the one-hot term.</p><p>Below we create a feature that conjoins two words. With eager allocation vectors have dimension <code>9</code> based on the <code>Words</code> domain defined above. With lazy allocation we can use, for example, vectors of dimension 2, provided that we only evaluate the term with two word pairs. Try changing <code>eager</code> to <code>true</code> and check whether the sparse vector has still dimension 2.</p>"
    }
  }, {
    "id" : 15,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val w1 = Words.Var\nval w2 = Words.Var\nval Vects = Vectors(2)\nval index = new SimpleFeatureIndex(Vects)\nval f = index.oneHot('f1, eager = false, w1, w2)\nSeq(f.eval(w1 := \"the\", w2 := \"cat\"), f.eval(w1 := \"cat\", w2 := \"sat\")) ",
      "extraFields" : {
        "aggregatedCells" : "[\"val Words = Seq(\\\"the\\\", \\\"cat\\\", \\\"sat\\\").toDom\\nval Vects = Vectors(Words.domainSize)\\n\\nval index = new SimpleFeatureIndex(Vects) //create index for the given domain\\nval w = Words.Var\\nval t = index.oneHot('f1, 2.1, w) //'f1 is the feature template name\\nt\",\"val v = t.eval(w := \\\"the\\\")\\nv\",\"v(0)\",\"t.eval(w := \\\"cat\\\")\",\"index.toMap(t.eval(w := \\\"sat\\\"))\",\"val Tags = Seq('Det, 'Noun, 'Verb).toDom\\nval Vects = Vectors(Words.domainSize * Tags.domainSize + 1) \\nval index = new SimpleFeatureIndex(Vects)\\ndef phi(w:Words.Term, y:Tags.Term) = {\\n  import index._\\n  oneHot('bias, y) + \\n  oneHot('lex, y, w) \\n}\\nval y = Tags.Var \\nval v = phi(w,y).eval(w := \\\"cat\\\", y := 'Noun)\\nv\",\"index.toMap(v)\"]"
      },
      "outputFormat" : "<div class=\"string-result\"><div class=\"asIterable List\"><span class=\"typeName\">List</span>\n<ol start=\"0\" class=\"fields\">\n  <li class=\"fieldValue\"><span class=\"asString String\">SparseIndexedTensor npos=1 sorted=0 ind=1,0,0,0 val=1.0,0.0,0.0,0.0</span></li>\n  <li class=\"fieldValue\"><span class=\"asString String\">SparseIndexedTensor npos=1 sorted=0 ind=0,0,0,0 val=1.0,0.0,0.0,0.0</span></li>\n</ol>\n</div></div>"
    }
  } ],
  "config" : { }
}
