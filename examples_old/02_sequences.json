{
  "name" : "Sequence Models",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Many problems involve predicting a linear sequence of labels or classes. For example,\nin NLP one likes to predict syntactic information on a per-token-basis. \nIn Wolfe such problems can be addressed by choosing sequences as possible worlds. ",
      "extraFields" : { },
      "outputFormat" : "<p>Many problems involve predicting a linear sequence of labels or classes. For example, in NLP one likes to predict syntactic information on a per-token-basis. In Wolfe such problems can be addressed by choosing sequences as possible worlds. </p>"
    }
  }, {
    "id" : 1,
    "compiler" : "heading3",
    "input" : {
      "sessionId" : null,
      "code" : "Chunking",
      "extraFields" : { },
      "outputFormat" : "<h3>Chunking</h3>"
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "[Chunking](http://en.wikipedia.org/wiki/Shallow_parsing) or shallow parsing\nbreaks up a sentence into its syntactic constituents such as noun phrases and\nverb phrases, but *without* detecting their tree-structure. The most\nsuccessful approaches to chunking cast the problem into a sequence \nlabelling task where each token is tagged with a label from the\nset \\\\( \\left\\\\{ \\mathrm{B},\\mathrm{I},\\mathrm{O} \\right\\\\} \n\\times \\left\\\\{\\mathrm{Noun},\\mathrm{Verb},\\ldots \\right\\\\} \\\\). \nHere the first part of the label indicates whether the token is\n__B__eginning a constituent, __I__nside one, or __O__utside. The second\npart indicates whether the constituent is a noun phrase, or verb phrase,\nor prepositional phrase etc. \n\nIn this example we use the data from the \n[CoNLL 2000 Shared Task](http://www.cnts.ua.ac.be/conll2000/chunking/) on chunking.\nThe model is a first order simple linear chain that captures the correlations between\nchunk labels and words at each token (line 10) and between consecutive labels\n(line 11). The empirical loss is a perceptron style loss function (line 15),\nand it is minimized in line 18 to train the model. \n\n\n",
      "extraFields" : { },
      "outputFormat" : "<p><a href=\"http://en.wikipedia.org/wiki/Shallow_parsing\">Chunking</a> or shallow parsing breaks up a sentence into its syntactic constituents such as noun phrases and verb phrases, but <em>without</em> detecting their tree-structure. The most successful approaches to chunking cast the problem into a sequence labelling task where each token is tagged with a label from the set <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-1\" role=\"math\" style=\"width: 14.11em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.729em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.729em 1000.003em 2.92em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mrow\" id=\"MathJax-Span-3\"><span class=\"mo\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Regular;\">{</span><span class=\"texatom\" id=\"MathJax-Span-5\"><span class=\"mrow\" id=\"MathJax-Span-6\"><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-family: STIXGeneral-Regular;\">B</span></span></span><span class=\"mo\" id=\"MathJax-Span-8\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-9\" style=\"padding-left: 0.182em;\"><span class=\"mrow\" id=\"MathJax-Span-10\"><span class=\"mi\" id=\"MathJax-Span-11\" style=\"font-family: STIXGeneral-Regular;\">I</span></span></span><span class=\"mo\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-13\" style=\"padding-left: 0.182em;\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Regular;\">O</span></span></span><span class=\"mo\" id=\"MathJax-Span-16\" style=\"font-family: STIXGeneral-Regular;\">}</span></span><span class=\"mo\" id=\"MathJax-Span-17\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">×</span><span class=\"mrow\" id=\"MathJax-Span-18\" style=\"padding-left: 0.241em;\"><span class=\"mo\" id=\"MathJax-Span-19\" style=\"vertical-align: 0.003em;\"><span style=\"font-family: STIXGeneral-Regular;\">{</span></span><span class=\"texatom\" id=\"MathJax-Span-20\"><span class=\"mrow\" id=\"MathJax-Span-21\"><span class=\"mi\" id=\"MathJax-Span-22\" style=\"font-family: STIXGeneral-Regular;\">N</span><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-family: STIXGeneral-Regular;\">o</span><span class=\"mi\" id=\"MathJax-Span-24\" style=\"font-family: STIXGeneral-Regular;\">u</span><span class=\"mi\" id=\"MathJax-Span-25\" style=\"font-family: STIXGeneral-Regular;\">n</span></span></span><span class=\"mo\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-27\" style=\"padding-left: 0.182em;\"><span class=\"mrow\" id=\"MathJax-Span-28\"><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Regular;\">V</span><span class=\"mi\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Regular;\">e</span><span class=\"mi\" id=\"MathJax-Span-31\" style=\"font-family: STIXGeneral-Regular;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-32\" style=\"font-family: STIXGeneral-Regular;\">b</span></span></span><span class=\"mo\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-34\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.182em;\">…</span><span class=\"mo\" id=\"MathJax-Span-35\" style=\"vertical-align: 0.003em;\"><span style=\"font-family: STIXGeneral-Regular;\">}</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.218em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-1\"> \\left\\{ \\mathrm{B},\\mathrm{I},\\mathrm{O} \\right\\} \\times \\left\\{\\mathrm{Noun},\\mathrm{Verb},\\ldots \\right\\} </script>. Here the first part of the label indicates whether the token is <strong>B</strong>eginning a constituent, <strong>I</strong>nside one, or <strong>O</strong>utside. The second part indicates whether the constituent is a noun phrase, or verb phrase, or prepositional phrase etc. </p><p>In this example we use the data from the <a href=\"http://www.cnts.ua.ac.be/conll2000/chunking/\">CoNLL 2000 Shared Task</a> on chunking. The model is a first order simple linear chain that captures the correlations between chunk labels and words at each token (line 10) and between consecutive labels (line 11). The empirical loss is a perceptron style loss function (line 15), and it is minimized in line 18 to train the model. </p>"
    }
  }, {
    "id" : 3,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import ml.wolfe.util.{Evaluator, NLP}\nimport NLP._\nimport ml.wolfe.macros.Library._\n\ndef sentences = all(Sentence)(seqs(all(Token)))\ndef obs(s: Sentence) = \n  s.copy(tokens = s.tokens.map(_.copy(chunk = hidden)))\ndef features(s: Sentence) = {\n  val n = s.tokens.size    \n  sum(0 until n) {i=>oneHot('o->s.tokens(i).word->s.tokens(i).chunk)} +\n  sum(0 until n-1) {i=>oneHot('p->s.tokens(i).chunk->s.tokens(i+1).chunk)}\n}\ndef f(w: Vector)(s: Sentence) = w dot features(s)\ndef h(w: Vector)(s: Sentence) = argmax(sentences st evidence(obs)(s)){f(w)}\ndef loss(data: Iterable[Sentence])(w: Vector) = \n  sum(data) { s => f(w)(h(w)(s)) - f(w)(s) }\nval train = NLP.conll2000TrainSample()\nval w = argmin(vectors) { loss(train) }\nval predicted = map(train) {h(w)}\nEvaluator.evaluate(\n  train.flatMap(_.tokens), \n  predicted.flatMap(_.tokens))(_.chunk)",
      "extraFields" : {
        "aggregatedCells" : "[]"
      },
      "outputFormat" : "<div class=\"string-result\"><span class=\"label label-danger\">Error!</span>\n<pre class=\"error\"><console>:55: error: object Evaluator is not a member of package ml.wolfe.util\n              import ml.wolfe.util.{Evaluator, NLP}\n                     ^\n<console>:56: error: not found: value NLP\n              import NLP._\n                     ^\n<console>:57: error: object macros is not a member of package ml.wolfe\n              import ml.wolfe.macros.Library._\n                              ^\n<console>:71: error: not found: value NLP\n              val train = NLP.conll2000TrainSample()\n                          ^\n<console>:72: error: not found: value argmin\n              val w = argmin(vectors) { loss(train) }\n                      ^\n<console>:72: error: not found: value vectors\n              val w = argmin(vectors) { loss(train) }\n                             ^\n<console>:70: error: overloaded method value sum with alternatives:\n  [T &lt;: ml.wolfe.term.Term[ml.wolfe.term.VarSeqDom[ml.wolfe.term.Dom]], Body &lt;: ml.wolfe.term.DoubleTerm](indices: T)(body: indices.domain.elementDom.Var =&gt; Body)ml.wolfe.term.FirstOrderSum[ml.wolfe.term.Dom,Body,ml.wolfe.term.Term[ml.wolfe.term.VarSeqDom[ml.wolfe.term.Dom]]] <and>\n  (length: ml.wolfe.term.IntTerm)(args: ml.wolfe.term.DoubleTerm*)ml.wolfe.term.DoubleTerm <and>\n  (args: ml.wolfe.term.DoubleTerm*)ml.wolfe.term.Sum <and>\n  [T](dom: Seq[T])(arg: T =&gt; ml.wolfe.term.DoubleTerm)ml.wolfe.term.Sum\n cannot be applied to (Iterable[ml.wolfe.nlp.Sentence])\n                sum(data) { s =&gt; f(w)(h(w)(s)) - f(w)(s) }\n                ^\n<console>:67: error: type Vector takes type parameters\n              def f(w: Vector)(s: Sentence) = w dot features(s)\n                       ^\n<console>:69: error: type Vector takes type parameters\n              def loss(data: Iterable[Sentence])(w: Vector) =\n                                                    ^\n<console>:59: error: not found: value all\n              def sentences = all(Sentence)(seqs(all(Token)))\n                              ^\n<console>:59: error: not found: value seqs\n              def sentences = all(Sentence)(seqs(all(Token)))\n                                            ^\n<console>:59: error: not found: value all\n              def sentences = all(Sentence)(seqs(all(Token)))\n                                                 ^\n<console>:68: error: type Vector takes type parameters\n              def h(w: Vector)(s: Sentence) = argmax(sentences st evidence(obs)(s)){f(w)}\n                       ^\n<console>:72: error: missing arguments for method loss;\nfollow this method with `_' if you want to treat it as a partially applied function\n              val w = argmin(vectors) { loss(train) }\n                                            ^\n<console>:76: error: not found: value Evaluator\n              Evaluator.evaluate(\n              ^\n<console>:61: error: not found: value chunk\n                s.copy(tokens = s.tokens.map(_.copy(chunk = hidden)))\n                                                    ^\n<console>:64: error: value chunk is not a member of ml.wolfe.nlp.Token\n                sum(0 until n) {i=&gt;oneHot('o-&gt;s.tokens(i).word-&gt;s.tokens(i).chunk)} +\n                                                                            ^\n<console>:65: error: value chunk is not a member of ml.wolfe.nlp.Token\n                sum(0 until n-1) {i=&gt;oneHot('p-&gt;s.tokens(i).chunk-&gt;s.tokens(i+1).chunk)}\n                                                            ^\n<console>:65: error: value chunk is not a member of ml.wolfe.nlp.Token\n                sum(0 until n-1) {i=&gt;oneHot('p-&gt;s.tokens(i).chunk-&gt;s.tokens(i+1).chunk)}\n                                                                                 ^\n</console></console></console></console></console></console></console></console></console></console></console></console></and></and></and></console></console></console></console></console></console></console></pre></div>"
    }
  } ],
  "config" : { }
}
