{
  "name" : "Welcome",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "code" : "Welcome to Wolfe! This documentation covers several aspects of Wolfe: \nan installation guide, an interactive tutorial on the core concepts \nof Wolfe, then a few examples of Wolfe applications. Please note that in many\ncases the shown code can be edited and run directly in the browser. Feel free\nto play around with these code snippets, but of course be considerate and don't\nrun large scale jobs on our server. \n\nBefore you go on, please note that Wolfe is in a __very early__ stage, far from a 1.0 release. It will generally\nbe slow, break unexpectedly, give weird or no warnings etc., so use it at your own risk.\n\n### What is Wolfe\n\nWolfe is a [probabilistic programming](http://probabilistic-programming.org/wiki/Home) \nframework aimed at simplifying the process of developing rich machine learning applications. \n\nWolfe acknowledges that at the heart of most machine learning (ML) applications are two concepts:\nmathematical models that assign probabilities or scores to hypotheses (or possible worlds),\nand mathematical operators that interact with these models, most prominently maximization and\nsummation.\n\n### Features\n\nWolfe is still in a very early stage, but\nalready comes comes with following features and abilities:\n\n* Inference\n    * Sum-Product Belief Propagation for marginal inference \n    * Max-Product Belief Propagation for MAP inference\n    * [Dual Decomposition](http://cs.nyu.edu/~dsontag/papers/SonGloJaa_optbook.pdf) \n    * [Alternating Directions Dual Decomposition](http://www.ark.cs.cmu.edu/AD3/)\n    * MCMC Sampling\n    * Hooks for combinatorial factors (such as tree constraints), Structured BP.  \n* Supervised Learning \n    * CRF style learning objectives\n    * Perceptron style learning objectives  \n    * State-of-the-art optimizers such as ADAGrad through the [FACTORIE](http://factorie.cs.umass.edu/) optimizer package. \n* Semisupervised Learning\n    * Latent CRF style learning\n\n### Name\nThe name Wolfe is a reference to [Philip Wolfe](http://en.wikipedia.org/wiki/Philip_Wolfe_\\(mathematician\\)), \na mathematician considered to be one of the founders of [mathematical programming](http://en.wikipedia.org/wiki/Mathematical_programming). \nThe name was chosen to reflect Wolfe's grounding in optimization: declaratively defining\nobjective and constraints, and optimizing such mathematical programs, is at the heart\nof Wolfe. Moreover, Wolfe relies on decomposition approaches such as the \n[Dantzig-Wolfe Decomposition](http://en.wikipedia.org/wiki/Dantzig%E2%80%93Wolfe_decomposition). \n\n### Contributors\nIn alphabetical order:\n\n* [Jan Noessner](http://dws.informatik.uni-mannheim.de/en/people/researchers/jan-noessner/\")\n* [Larysa Visengeriyeva](https://twitter.com/visenger)\n* [Luke Hewitt](http://lukehewitt.co.uk)\n* [Sameer Singh](http://people.cs.umass.edu/~sameer/)\n* [Sebastian Riedel](http://www.riedelcastro.org)\n* [Tim Rockt√§schel](http://rockt.github.io/)\n* [Vivek Srikumar](http://svivek.com/)\n\n\n",
      "outputFormat" : "html",
      "extraFields" : null
    }
  } ]
}
