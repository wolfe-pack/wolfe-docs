{
  "name" : "Linear Chains",
  "cells" : [ {
    "id" : 0,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import ml.wolfe.nlp.io.CoNLLReader\nval trainSents = new CoNLLReader(\"ml/wolfe/datasets/conll2000/train.txt\", \" \")\nval testSents = new CoNLLReader(\"ml/wolfe/datasets/conll2000/test.txt\", \" \")\ndef getChunkTags(s: Sentence) = {\n  val chunkTags = Array.fill(s.tokens.length)(\"O\")\n  s.ie.entityMentions.foreach { chunk =>\n    chunkTags(chunk.start) = if (chunk.label == \"O\") \"O\" else \"B-\" + chunk.label\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \"I-\" + chunk.label\n  }\n  collection.immutable.IndexedSeq(chunkTags: _*)\n}\n\ndef getPosTags(s: Sentence) = {\n  val posTags = s.tokens.map(_.posTag)\n  collection.immutable.IndexedSeq(posTags: _*)\n}\n\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\n  s.tokens.map(_.word.toString) -> getPosTags(s)\n}\n\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\nval test = testSents.take(10).map(toInstance).toIndexedSeq\n\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\n1",
      "extraFields" : {
        "hide" : "true",
        "aggregatedCells" : "[]"
      },
      "outputFormat" : "<div class=\"string-result\"><span class=\"asString String\">1</span></div>"
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We like to predict the part-of-speech labels of tokens in text. We are given a `train` and `test` set, each with pairs of token sequences and tag sequences. \n\n#### Data\n\nBelow we inspect the second training instance and print pairs of words and tags, for the first 5 tokens. ",
      "extraFields" : { },
      "outputFormat" : "<p>We like to predict the part-of-speech labels of tokens in text. We are given a <code>train</code> and <code>test</code> set, each with pairs of token sequences and tag sequences. </p><h4>Data</h4><p>Below we inspect the second training instance and print pairs of words and tags, for the first 5 tokens. </p>"
    }
  }, {
    "id" : 2,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "(train(1)._1 zip train(1)._2) take 5",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\"]"
      },
      "outputFormat" : "<div class=\"text-center\"><i class=\"fa fa-refresh fa-spin fa-lg\"></i></div>"
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "In the following we will introduce a few models to address this task, starting from standard linear chain structured prediction models with linear potentials to models with non-linear potentials (e.g., using a neural network within each potential).\n\n### A _Linear_ Linear Chain Model\nWe will first attempt to model this problem using a \"traditional\" first order linear chain model. These models score the sequence labels according to how well each individual label matches the sentence observations (for example, the word at the given token), and how consistent every two consecutive labels in the sequence are (for example, do determiners precede nouns). The scoring function extracts a feature representation for each single label, and for each consecutive pair of labels, and then multiplies these representations with a weight vector. \n\n\nMore formally, given a sequence of words \\\\(\\mathbf{x}\\\\), a sequence of labels \\\\(\\mathbf{y}\\\\), and a weight vector \\\\(\\boldsymbol{\\theta}\\\\), the model assigns the following score: \n$$\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\params}{\\boldsymbol{\\theta}}\n\\newcommand{\\feats}{\\boldsymbol{\\phi}}\ns_{\\params}(\\x,\\y) = \\sum_i^{|\\x|} \\langle \\params,\\feats_{\\text{local}}(\\x,y_i,i) \\rangle + \\sum_i^{|x|-1} \\langle \\params, \\feats_{\\text{transition}}(y_i,y_{i+1}) \\rangle \n$$\nThe model is linear both in the sense that the structure of the model is sequential, and that the scoring functions are linear with respect to the weights. \n\n#### Domains\nTo create this model in wolfe we first need to setup the [domains](/template/wolfe/wolfe-docs/concepts/02_terms) of values the model is concerned with: tags, words, and sequences of such values. Our first step is to collect the relevant values from the dataset. This includes upper bounds on the number of elements in each sequence.\n",
      "extraFields" : { },
      "outputFormat" : "<p>In the following we will introduce a few models to address this task, starting from standard linear chain structured prediction models with linear potentials to models with non-linear potentials (e.g., using a neural network within each potential).</p><h3>A <em>Linear</em> Linear Chain Model</h3><p>We will first attempt to model this problem using a \"traditional\" first order linear chain model. These models score the sequence labels according to how well each individual label matches the sentence observations (for example, the word at the given token), and how consistent every two consecutive labels in the sequence are (for example, do determiners precede nouns). The scoring function extracts a feature representation for each single label, and for each consecutive pair of labels, and then multiplies these representations with a weight vector. </p><p>More formally, given a sequence of words <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-14\" role=\"math\" style=\"width: 0.658em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.539em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.908em 1000.003em 2.741em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-15\"><span class=\"texatom\" id=\"MathJax-Span-16\"><span class=\"mrow\" id=\"MathJax-Span-17\"><span class=\"mi\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral; font-weight: bold;\">x</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.718em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-4\">\\mathbf{x}</script>, a sequence of labels <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-19\" role=\"math\" style=\"width: 0.658em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.539em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.908em 1000.003em 2.92em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-20\"><span class=\"texatom\" id=\"MathJax-Span-21\"><span class=\"mrow\" id=\"MathJax-Span-22\"><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.932em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-5\">\\mathbf{y}</script>, and a weight vector <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-24\" role=\"math\" style=\"width: 0.658em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.539em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.61em 1000.003em 2.682em -999.997em); top: -2.497em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-25\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.004em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-6\">\\boldsymbol{\\theta}</script>, the model assigns the following score: <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-27\" role=\"math\" style=\"width: 28.336em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 23.574em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.598em 1000.003em 3.991em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-28\"><span class=\"msubsup\" id=\"MathJax-Span-29\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"texatom\" id=\"MathJax-Span-31\"><span class=\"mrow\" id=\"MathJax-Span-32\"><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-34\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-35\"><span class=\"mrow\" id=\"MathJax-Span-36\"><span class=\"mi\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral; font-weight: bold;\">x</span></span></span><span class=\"mo\" id=\"MathJax-Span-38\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-39\" style=\"padding-left: 0.182em;\"><span class=\"mrow\" id=\"MathJax-Span-40\"><span class=\"mi\" id=\"MathJax-Span-41\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span class=\"mo\" id=\"MathJax-Span-42\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-43\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.301em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-44\" style=\"padding-left: 0.301em;\"><span style=\"display: inline-block; position: relative; width: 1.313em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.86em 1000.003em 4.646em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mo\" id=\"MathJax-Span-45\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.533em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.289em -999.997em); top: -2.854em; left: 0.539em;\"><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.217em 1000.003em 4.17em -999.997em); top: -5.176em; left: 0.36em;\"><span class=\"texatom\" id=\"MathJax-Span-47\"><span class=\"mrow\" id=\"MathJax-Span-48\"><span class=\"texatom\" id=\"MathJax-Span-49\"><span class=\"mrow\" id=\"MathJax-Span-50\"><span class=\"mo\" id=\"MathJax-Span-51\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">|</span></span></span><span class=\"texatom\" id=\"MathJax-Span-52\"><span class=\"mrow\" id=\"MathJax-Span-53\"><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-weight: bold;\">x</span></span></span><span class=\"texatom\" id=\"MathJax-Span-55\"><span class=\"mrow\" id=\"MathJax-Span-56\"><span class=\"mo\" id=\"MathJax-Span-57\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">|</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-58\" style=\"font-family: STIXGeneral-Regular;\">⟨</span><span class=\"mi\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span><span class=\"mo\" id=\"MathJax-Span-60\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-61\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 2.146em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.098em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-62\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">ϕ</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.658em;\"><span class=\"texatom\" id=\"MathJax-Span-63\"><span class=\"mrow\" id=\"MathJax-Span-64\"><span class=\"mtext\" id=\"MathJax-Span-65\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">local</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-67\"><span class=\"mrow\" id=\"MathJax-Span-68\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-family: STIXGeneral; font-weight: bold;\">x</span></span></span><span class=\"mo\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-71\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 0.717em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-74\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-75\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.182em;\">i</span><span class=\"mo\" id=\"MathJax-Span-76\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-77\" style=\"font-family: STIXGeneral-Regular;\">⟩</span><span class=\"mo\" id=\"MathJax-Span-78\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"munderover\" id=\"MathJax-Span-79\" style=\"padding-left: 0.241em;\"><span style=\"display: inline-block; position: relative; width: 1.432em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.86em 1000.003em 4.646em -999.997em); top: -3.985em; left: 0.063em;\"><span class=\"mo\" id=\"MathJax-Span-80\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.533em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.289em -999.997em); top: -2.854em; left: 0.598em;\"><span class=\"mi\" id=\"MathJax-Span-81\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.217em 1000.003em 4.17em -999.997em); top: -5.176em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-82\"><span class=\"mrow\" id=\"MathJax-Span-83\"><span class=\"texatom\" id=\"MathJax-Span-84\"><span class=\"mrow\" id=\"MathJax-Span-85\"><span class=\"mo\" id=\"MathJax-Span-86\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-87\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-88\"><span class=\"mrow\" id=\"MathJax-Span-89\"><span class=\"mo\" id=\"MathJax-Span-90\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">|</span></span></span><span class=\"mo\" id=\"MathJax-Span-91\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-92\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-family: STIXGeneral-Regular;\">⟨</span><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span><span class=\"mo\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-96\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 3.455em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.098em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-97\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">ϕ</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.658em;\"><span class=\"texatom\" id=\"MathJax-Span-98\"><span class=\"mrow\" id=\"MathJax-Span-99\"><span class=\"mtext\" id=\"MathJax-Span-100\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">transition</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-102\"><span style=\"display: inline-block; position: relative; width: 0.717em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"mi\" id=\"MathJax-Span-104\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-105\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-106\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 1.551em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"texatom\" id=\"MathJax-Span-108\"><span class=\"mrow\" id=\"MathJax-Span-109\"><span class=\"mi\" id=\"MathJax-Span-110\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-111\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-112\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-113\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-114\" style=\"font-family: STIXGeneral-Regular;\">⟩</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 3.789em; vertical-align: -1.568em;\"></span></span></nobr></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-7\"> \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\params}{\\boldsymbol{\\theta}} \\newcommand{\\feats}{\\boldsymbol{\\phi}} s_{\\params}(\\x,\\y) = \\sum_i^{|\\x|} \\langle \\params,\\feats_{\\text{local}}(\\x,y_i,i) \\rangle + \\sum_i^{|x|-1} \\langle \\params, \\feats_{\\text{transition}}(y_i,y_{i+1}) \\rangle </script> The model is linear both in the sense that the structure of the model is sequential, and that the scoring functions are linear with respect to the weights. </p><h4>Domains</h4><p>To create this model in wolfe we first need to setup the <a href=\"/template/wolfe/wolfe-docs/concepts/02_terms\">domains</a> of values the model is concerned with: tags, words, and sequences of such values. Our first step is to collect the relevant values from the dataset. This includes upper bounds on the number of elements in each sequence.</p>"
    }
  }, {
    "id" : 4,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val words = (train ++ test).flatMap(_._1).distinct\nval tags = (train ++ test).flatMap(_._2).distinct\nval maxLength = (train ++ test).map(_._1.length).max",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Notice that you don't need to get the values of domains in this way. Next you can create wolfe domain objects from these collections (notice we use upper case names for these by convention).",
      "extraFields" : { },
      "outputFormat" : "<p>Notice that you don't need to get the values of domains in this way. Next you can create wolfe domain objects from these collections (notice we use upper case names for these by convention).</p>"
    }
  }, {
    "id" : 6,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val Words = words.toDom \nval Tags = tags.toDom\nval Y = Seqs(Tags, 0, maxLength)\nval X = Seqs(Words, 0, maxLength)",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 7,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Next we create the domain of parameter/feature vectors. For this we primarily need to choose a dimension. The easiest is to choose a number well above what we expect to be the number of parameters. If we overshoot this is not too bad.",
      "extraFields" : { },
      "outputFormat" : "<p>Next we create the domain of parameter/feature vectors. For this we primarily need to choose a dimension. The easiest is to choose a number well above what we expect to be the number of parameters. If we overshoot this is not too bad.</p>"
    }
  }, {
    "id" : 8,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val Thetas = Vectors(100000)",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Feature Functions\nWe can now start defining the model by first specifying the feature functions \\\\(\\feats_{\\text{local}}\\\\) and \\\\(\\feats_{\\text{transition}}\\\\). We want these feature functions to return [sparse feature vectors](template/wolfe/wolfe-docs/concepts/05_sparse_feats), active at integer indices corresponding to certain features of the input. For this we need a feature index that maintains the mapping from features (such as pairs of tags and words) to integer indices.   \n\n",
      "extraFields" : { },
      "outputFormat" : "<h4>Feature Functions</h4><p>We can now start defining the model by first specifying the feature functions <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-115\" role=\"math\" style=\"width: 2.682em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.205em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.313em 1000.003em 2.562em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-116\"><span class=\"msubsup\" id=\"MathJax-Span-117\"><span style=\"display: inline-block; position: relative; width: 2.146em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.098em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-118\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">ϕ</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.658em;\"><span class=\"texatom\" id=\"MathJax-Span-119\"><span class=\"mrow\" id=\"MathJax-Span-120\"><span class=\"mtext\" id=\"MathJax-Span-121\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">local</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.218em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-8\">\\feats_{\\text{local}}</script> and <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-122\" role=\"math\" style=\"width: 4.229em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.515em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.313em 1000.003em 2.562em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-123\"><span class=\"msubsup\" id=\"MathJax-Span-124\"><span style=\"display: inline-block; position: relative; width: 3.455em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.098em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-125\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">ϕ</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.658em;\"><span class=\"texatom\" id=\"MathJax-Span-126\"><span class=\"mrow\" id=\"MathJax-Span-127\"><span class=\"mtext\" id=\"MathJax-Span-128\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">transition</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.218em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-9\">\\feats_{\\text{transition}}</script>. We want these feature functions to return <a href=\"template/wolfe/wolfe-docs/concepts/05_sparse_feats\">sparse feature vectors</a>, active at integer indices corresponding to certain features of the input. For this we need a feature index that maintains the mapping from features (such as pairs of tags and words) to integer indices. </p>"
    }
  }, {
    "id" : 10,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val index = new SimpleFeatureIndex(Thetas)",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 11,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "The local feature function will, for now, only consider a bias feature, and the conjunction of \\\\(x_i\\\\) and label \\\\(y_i\\\\) at a particular token index \\\\(i\\\\)",
      "extraFields" : { },
      "outputFormat" : "<p>The local feature function will, for now, only consider a bias feature, and the conjunction of <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-1\" role=\"math\" style=\"width: 0.955em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.61em 1000.003em 2.562em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"msubsup\" id=\"MathJax-Span-3\"><span style=\"display: inline-block; position: relative; width: 0.717em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"mi\" id=\"MathJax-Span-5\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.861em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-1\">x_i</script> and label <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-6\" role=\"math\" style=\"width: 0.955em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.61em 1000.003em 2.562em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-7\"><span class=\"msubsup\" id=\"MathJax-Span-8\"><span style=\"display: inline-block; position: relative; width: 0.717em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-9\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"mi\" id=\"MathJax-Span-10\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.932em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-2\">y_i</script> at a particular token index <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-11\" role=\"math\" style=\"width: 0.36em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.301em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.729em 1000.003em 2.741em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-12\"><span class=\"mi\" id=\"MathJax-Span-13\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.932em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-3\">i</script></p>"
    }
  }, {
    "id" : 12,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\n  index.oneHot('bias, y) + \n  index.oneHot('word, y, x(i))    \n}",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 13,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We can test this feature function by instantiating it with some concrete (constant) terms and evaluating it to a vector. \n",
      "extraFields" : { },
      "outputFormat" : "<p>We can test this feature function by instantiating it with some concrete (constant) terms and evaluating it to a vector. </p>"
    }
  }, {
    "id" : 14,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val t = local(X.Const(IndexedSeq(\"the\",\"Chancellor\")), Tags.Const(\"NNP\"), 1)\nt.eval() //For an explicit map representation use index.toMap(t.eval())",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 15,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We do the same thing for the transition features. ",
      "extraFields" : { },
      "outputFormat" : "<p>We do the same thing for the transition features. </p>"
    }
  }, {
    "id" : 16,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def transition(y1:Tags.Term, y2:Tags.Term) = {\n  index.oneHot('trans, y1, y2)    \n}",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 17,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Linear Model\nWe are now ready to define the actual model, with an almost one-to-one correspondence to the math we wrote above. \n",
      "extraFields" : { },
      "outputFormat" : "<h4>Linear Model</h4><p>We are now ready to define the actual model, with an almost one-to-one correspondence to the math we wrote above. </p>"
    }
  }, {
    "id" : 18,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\n} subjectTo (y.length === x.length)",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 19,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Notice that thhe math missed one part of aspect of the model: the length of `y` should equal the length of `x`. Often such constraints are either kept implicit, or explictly stated in optimization problems that involve the model. We like to directly tie this constraint to the model, hence the `subjectTo` expression that makes sure that the model returns \\\\(-\\infty\\\\) if the length constraint is violated. \n\nIn addition we want to inform the learn and inference engine to use the Max-Product algorithm on this model whenever maximization is needed. This leads to the following definition.",
      "extraFields" : { },
      "outputFormat" : "<p>Notice that thhe math missed one part of aspect of the model: the length of <code>y</code> should equal the length of <code>x</code>. Often such constraints are either kept implicit, or explictly stated in optimization problems that involve the model. We like to directly tie this constraint to the model, hence the <code>subjectTo</code> expression that makes sure that the model returns <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-129\" role=\"math\" style=\"width: 2.027em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.67em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.967em 1000.003em 2.741em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-130\"><span class=\"mo\" id=\"MathJax-Span-131\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mi\" id=\"MathJax-Span-132\" style=\"font-family: STIXGeneral-Regular;\">∞</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.646em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-10\">-\\infty</script> if the length constraint is violated. </p><p>In addition we want to inform the learn and inference engine to use the Max-Product algorithm on this model whenever maximization is needed. This leads to the following definition.</p>"
    }
  }, {
    "id" : 20,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\"]",
        "hide_output" : "true"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 21,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Training\nTraining the model means estimating good values for the parameter vector \\\\(\\params\\\\). In wolfe this can be achieved by defining a _learning objective_ (or negated training loss) and estimating its argmax. Our first learning objective will be a (negative) hinge-loss with zero margin. \n\nMathematically the zero-margin hinge-loss objective, for a given training set \\\\(\\mathcal{D}\\\\) is defined as follows:\n$$\no(\\params) = \\sum_{(\\x_i,\\y_i) \\in \\mathcal{D}} s_{\\params}(\\x_i,\\y_i) - \\max_{\\y\\in\\mathcal{Y}} s_{\\params}(\\x_i,\\y)    \n$$\nIntuitively, this objective encourages the score of the gold/true solution \\\\(\\y_i\\\\) to be the highest score. In this case the objective for instance \\\\(i\\\\) will be 0. You will note that this objective has a trivial solution (the zero parameter vector); however, running SGD (or variants) on it yields the perceptron algorithm, which will return a non-trivial solution.\n\nIn wolfe we can define this objective symbolically using terms. We can almost directly follow the math. As first step we need to convert our training set `train` into a term to represent \\\\(\\mathcal{D}\\\\). \n",
      "extraFields" : { },
      "outputFormat" : "<h4>Training</h4><p>Training the model means estimating good values for the parameter vector <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-133\" role=\"math\" style=\"width: 0.658em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.539em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.61em 1000.003em 2.682em -999.997em); top: -2.497em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-134\"><span class=\"mi\" id=\"MathJax-Span-135\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.004em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-11\">\\params</script>. In wolfe this can be achieved by defining a <em>learning objective</em> (or negated training loss) and estimating its argmax. Our first learning objective will be a (negative) hinge-loss with zero margin. </p><p>Mathematically the zero-margin hinge-loss objective, for a given training set <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-136\" role=\"math\" style=\"width: 1.015em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.146em 1000.003em 3.158em -999.997em); top: -2.973em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-137\"><span class=\"texatom\" id=\"MathJax-Span-138\"><span class=\"mrow\" id=\"MathJax-Span-139\"><span class=\"mi\" id=\"MathJax-Span-140\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.979em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.932em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-12\">\\mathcal{D}</script> is defined as follows: <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-141\" role=\"math\" style=\"width: 18.872em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 15.717em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.432em 1000.003em 4.17em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-142\"><span class=\"mi\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Italic;\">o</span><span class=\"mo\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span><span class=\"mo\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.301em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-148\" style=\"padding-left: 0.301em;\"><span style=\"display: inline-block; position: relative; width: 2.801em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.86em 1000.003em 4.646em -999.997em); top: -3.985em; left: 0.777em;\"><span class=\"mo\" id=\"MathJax-Span-149\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.533em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.467em -999.997em); top: -2.854em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-150\"><span class=\"mrow\" id=\"MathJax-Span-151\"><span class=\"mo\" id=\"MathJax-Span-152\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-153\"><span style=\"display: inline-block; position: relative; width: 0.539em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.515em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-154\"><span class=\"mrow\" id=\"MathJax-Span-155\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-weight: bold;\">x</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.866em; left: 0.36em;\"><span class=\"mi\" id=\"MathJax-Span-157\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-158\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-159\"><span style=\"display: inline-block; position: relative; width: 0.539em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.515em 1000.003em 4.289em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-160\"><span class=\"mrow\" id=\"MathJax-Span-161\"><span class=\"mi\" id=\"MathJax-Span-162\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.36em;\"><span class=\"mi\" id=\"MathJax-Span-163\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-164\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-165\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"texatom\" id=\"MathJax-Span-166\"><span class=\"mrow\" id=\"MathJax-Span-167\"><span class=\"mi\" id=\"MathJax-Span-168\" style=\"font-size: 70.7%; font-family: STIXNonUnicode-Italic;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-169\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-170\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"texatom\" id=\"MathJax-Span-171\"><span class=\"mrow\" id=\"MathJax-Span-172\"><span class=\"mi\" id=\"MathJax-Span-173\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-174\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-175\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-176\"><span class=\"mrow\" id=\"MathJax-Span-177\"><span class=\"mi\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral; font-weight: bold;\">x</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.479em;\"><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-180\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-181\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-182\"><span class=\"mrow\" id=\"MathJax-Span-183\"><span class=\"mi\" id=\"MathJax-Span-184\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 0.479em;\"><span class=\"mi\" id=\"MathJax-Span-185\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-186\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-187\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">−</span><span class=\"munderover\" id=\"MathJax-Span-188\" style=\"padding-left: 0.241em;\"><span style=\"display: inline-block; position: relative; width: 1.729em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mo\" id=\"MathJax-Span-189\" style=\"font-family: STIXGeneral-Regular;\">max</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.408em -999.997em); top: -3.271em; left: 0.182em;\"><span class=\"texatom\" id=\"MathJax-Span-190\"><span class=\"mrow\" id=\"MathJax-Span-191\"><span class=\"texatom\" id=\"MathJax-Span-192\"><span class=\"mrow\" id=\"MathJax-Span-193\"><span class=\"mi\" id=\"MathJax-Span-194\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span class=\"mo\" id=\"MathJax-Span-195\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"texatom\" id=\"MathJax-Span-196\"><span class=\"mrow\" id=\"MathJax-Span-197\"><span class=\"mi\" id=\"MathJax-Span-198\" style=\"font-size: 70.7%; font-family: STIXNonUnicode-Italic;\"><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-199\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-200\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"texatom\" id=\"MathJax-Span-201\"><span class=\"mrow\" id=\"MathJax-Span-202\"><span class=\"mi\" id=\"MathJax-Span-203\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-204\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-205\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-206\"><span class=\"mrow\" id=\"MathJax-Span-207\"><span class=\"mi\" id=\"MathJax-Span-208\" style=\"font-family: STIXGeneral; font-weight: bold;\">x</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.479em;\"><span class=\"mi\" id=\"MathJax-Span-209\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-210\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-211\" style=\"padding-left: 0.182em;\"><span class=\"mrow\" id=\"MathJax-Span-212\"><span class=\"mi\" id=\"MathJax-Span-213\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span class=\"mo\" id=\"MathJax-Span-214\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 3.004em; vertical-align: -1.782em;\"></span></span></nobr></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-13\"> o(\\params) = \\sum_{(\\x_i,\\y_i) \\in \\mathcal{D}} s_{\\params}(\\x_i,\\y_i) - \\max_{\\y\\in\\mathcal{Y}} s_{\\params}(\\x_i,\\y) </script> Intuitively, this objective encourages the score of the gold/true solution <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-215\" role=\"math\" style=\"width: 1.015em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.551em 1000.003em 2.622em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-216\"><span class=\"msubsup\" id=\"MathJax-Span-217\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-218\"><span class=\"mrow\" id=\"MathJax-Span-219\"><span class=\"mi\" id=\"MathJax-Span-220\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 0.479em;\"><span class=\"mi\" id=\"MathJax-Span-221\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.004em; vertical-align: -0.354em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-14\">\\y_i</script> to be the highest score. In this case the objective for instance <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-222\" role=\"math\" style=\"width: 0.36em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.301em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.729em 1000.003em 2.741em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-223\"><span class=\"mi\" id=\"MathJax-Span-224\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.932em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-15\">i</script> will be 0. You will note that this objective has a trivial solution (the zero parameter vector); however, running SGD (or variants) on it yields the perceptron algorithm, which will return a non-trivial solution.</p><p>In wolfe we can define this objective symbolically using terms. We can almost directly follow the math. As first step we need to convert our training set <code>train</code> into a term to represent <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-225\" role=\"math\" style=\"width: 1.015em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.146em 1000.003em 3.158em -999.997em); top: -2.973em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-226\"><span class=\"texatom\" id=\"MathJax-Span-227\"><span class=\"mrow\" id=\"MathJax-Span-228\"><span class=\"mi\" id=\"MathJax-Span-229\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.979em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.932em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-16\">\\mathcal{D}</script>. </p>"
    }
  }, {
    "id" : 22,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val D = Seqs(Pairs(X,Y),train.length).Const(train)",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 23,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We can now define the function \\\\(o\\\\) in wolfe.",
      "extraFields" : { },
      "outputFormat" : "<p>We can now define the function <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-230\" role=\"math\" style=\"width: 0.658em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.539em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.967em 1000.003em 2.741em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-231\"><span class=\"mi\" id=\"MathJax-Span-232\" style=\"font-family: STIXGeneral-Italic;\">o</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.718em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-17\">o</script> in wolfe.</p>"
    }
  }, {
    "id" : 24,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def o(theta:Thetas.Term) = \n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 25,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We can now learn by finding a vector \\\\(\\params^*\\\\)that maximizes this objective. This can be done using the `argmax` operator in wolfe. ",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(50).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n1\",\"(train(1)._1.take(5) zip train(1)._2.take(5))\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} \",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) subjectTo (y.length === x.length) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\",\"def o(theta:Thetas.Term) = \\n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}\"]"
      },
      "outputFormat" : "<p>We can now learn by finding a vector <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-233\" role=\"math\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.015em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.313em 1000.003em 2.384em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-234\"><span class=\"msubsup\" id=\"MathJax-Span-235\"><span style=\"display: inline-block; position: relative; width: 0.955em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.098em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-236\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.342em; left: 0.479em;\"><span class=\"mo\" id=\"MathJax-Span-237\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.004em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-18\">\\params^*</script>that maximizes this objective. This can be done using the <code>argmax</code> operator in wolfe. </p>"
    }
  }, {
    "id" : 26,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val params = AdaGradParameters(5, 0.1)\nval thetaStar = argmax(Thetas)(theta => o(theta)) by adaGrad(params) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\",\"def o(theta:Thetas.Term) = \\n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}\"]",
        "hide_output" : "true"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 27,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "At this point `thetaStar` is a term, a symbolic definition of the optimal weights, but no actual optimization has taken place. We could delay this further until the weights are needed for an actual `eval` call, but this may lead to long computations when we don't expect it. Instead we `precalculate` the value of this term now.  ",
      "extraFields" : { },
      "outputFormat" : "<p>At this point <code>thetaStar</code> is a term, a symbolic definition of the optimal weights, but no actual optimization has taken place. We could delay this further until the weights are needed for an actual <code>eval</code> call, but this may lead to long computations when we don't expect it. Instead we <code>precalculate</code> the value of this term now. </p>"
    }
  }, {
    "id" : 28,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val thetaStar = { argmax(Thetas)(theta => o(theta)) by adaGrad(params) }.precalculate",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\",\"def o(theta:Thetas.Term) = \\n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}\",\"val params = AdaGradParameters(5, 0.1)\\nval thetaStar = argmax(Thetas)(theta => o(theta)) by adaGrad(params) \"]",
        "hide_output" : "true"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 29,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Prediction\nWith the weight vector \\\\(\\params^*\\\\) we can now define the prediction function. This function is defined as\n$$\nh(\\x) = \\arg \\max_{\\y \\in Y} s_{\\params^*}(\\x,\\y)\n$$\nand can be formulated similarily in wolfe:",
      "extraFields" : { },
      "outputFormat" : "<h4>Prediction</h4><p>With the weight vector <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-238\" role=\"math\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.015em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.313em 1000.003em 2.384em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-239\"><span class=\"msubsup\" id=\"MathJax-Span-240\"><span style=\"display: inline-block; position: relative; width: 0.955em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.098em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-241\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.342em; left: 0.479em;\"><span class=\"mo\" id=\"MathJax-Span-242\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.004em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-19\">\\params^*</script> we can now define the prediction function. This function is defined as <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-243\" role=\"math\" style=\"width: 11.551em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.586em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.729em 1000.003em 3.693em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-244\"><span class=\"mi\" id=\"MathJax-Span-245\" style=\"font-family: STIXGeneral-Italic;\">h</span><span class=\"mo\" id=\"MathJax-Span-246\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-247\"><span class=\"mrow\" id=\"MathJax-Span-248\"><span class=\"mi\" id=\"MathJax-Span-249\" style=\"font-family: STIXGeneral; font-weight: bold;\">x</span></span></span><span class=\"mo\" id=\"MathJax-Span-250\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-251\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.301em;\">=</span><span class=\"mi\" id=\"MathJax-Span-252\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.301em;\">arg</span><span class=\"mo\" id=\"MathJax-Span-253\"></span><span class=\"munderover\" id=\"MathJax-Span-254\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 1.729em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mo\" id=\"MathJax-Span-255\" style=\"font-family: STIXGeneral-Regular;\">max</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.408em -999.997em); top: -3.271em; left: 0.241em;\"><span class=\"texatom\" id=\"MathJax-Span-256\"><span class=\"mrow\" id=\"MathJax-Span-257\"><span class=\"texatom\" id=\"MathJax-Span-258\"><span class=\"mrow\" id=\"MathJax-Span-259\"><span class=\"mi\" id=\"MathJax-Span-260\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span class=\"mo\" id=\"MathJax-Span-261\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-262\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-263\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 1.134em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"texatom\" id=\"MathJax-Span-265\"><span class=\"mrow\" id=\"MathJax-Span-266\"><span class=\"msubsup\" id=\"MathJax-Span-267\"><span style=\"display: inline-block; position: relative; width: 0.658em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-268\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.283em; left: 0.36em;\"><span class=\"mo\" id=\"MathJax-Span-269\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-270\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-271\"><span class=\"mrow\" id=\"MathJax-Span-272\"><span class=\"mi\" id=\"MathJax-Span-273\" style=\"font-family: STIXGeneral; font-weight: bold;\">x</span></span></span><span class=\"mo\" id=\"MathJax-Span-274\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-275\" style=\"padding-left: 0.182em;\"><span class=\"mrow\" id=\"MathJax-Span-276\"><span class=\"mi\" id=\"MathJax-Span-277\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span class=\"mo\" id=\"MathJax-Span-278\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 2.075em; vertical-align: -1.211em;\"></span></span></nobr></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-20\"> h(\\x) = \\arg \\max_{\\y \\in Y} s_{\\params^*}(\\x,\\y) </script> and can be formulated similarily in wolfe:</p>"
    }
  }, {
    "id" : 30,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def h(x:X.Term) = argmax(Y) {y => model(thetaStar)(x,y)}",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\",\"def o(theta:Thetas.Term) = \\n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}\",\"val params = AdaGradParameters(5, 0.1)\\nval thetaStar = argmax(Thetas)(theta => o(theta)) by adaGrad(params) \",\"val thetaStar = { argmax(Thetas)(theta => o(theta)) by adaGrad(params) }.precalculate\"]",
        "hide_output" : "true"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 31,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "To apply the predictor to an inpux _value_ we need to convert the value to a term (constant), apply the predictor function to this constant, and then evaluate the term.",
      "extraFields" : { },
      "outputFormat" : "<p>To apply the predictor to an inpux <em>value</em> we need to convert the value to a term (constant), apply the predictor function to this constant, and then evaluate the term.</p>"
    }
  }, {
    "id" : 32,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val yStar = h(X.Const(test(0)._1)).eval()\n(yStar zip test(0)._1) take 4",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\",\"def o(theta:Thetas.Term) = \\n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}\",\"val params = AdaGradParameters(5, 0.1)\\nval thetaStar = argmax(Thetas)(theta => o(theta)) by adaGrad(params) \",\"val thetaStar = { argmax(Thetas)(theta => o(theta)) by adaGrad(params) }.precalculate\",\"def h(x:X.Term) = argmax(Y) {y => model(thetaStar)(x,y)}\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 33,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "In practice you will often want to use a trained model in the above fashion: some input _value_ is observed, and some prediction is made based on this observation. Doing this repeatedly as shown above can be quite inefficient, as for each prediction a new `eval` call is necessary, which requires memory allocation, term tree optimizations etc. To simplify this use case, wolfe provides the `fun` method which can create efficient functions that map values to the results of some term tree. ",
      "extraFields" : { },
      "outputFormat" : "<p>In practice you will often want to use a trained model in the above fashion: some input <em>value</em> is observed, and some prediction is made based on this observation. Doing this repeatedly as shown above can be quite inefficient, as for each prediction a new <code>eval</code> call is necessary, which requires memory allocation, term tree optimizations etc. To simplify this use case, wolfe provides the <code>fun</code> method which can create efficient functions that map values to the results of some term tree. </p>"
    }
  }, {
    "id" : 34,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val predict = fun(X) {x => h(x)}\n(predict(test(0)._1) zip test(0)._1) take 4",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\",\"def o(theta:Thetas.Term) = \\n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}\",\"val params = AdaGradParameters(5, 0.1)\\nval thetaStar = argmax(Thetas)(theta => o(theta)) by adaGrad(params) \",\"val thetaStar = { argmax(Thetas)(theta => o(theta)) by adaGrad(params) }.precalculate\",\"def h(x:X.Term) = argmax(Y) {y => model(thetaStar)(x,y)}\",\"val yStar = h(X.Const(test(0)._1)).eval()\\n(yStar zip test(0)._1) take 4\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 35,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Margin-Based Learning (Structured SVM)\nWe mentioned that our training objective enforces no margin. This may lead to less robust models. To overcome this we can simply change the training objective to incorporate the notion of [loss-augmented inference](). More concretely, instead of ony making sure that the correct solution \\\\(\\y_i\\\\) for training instance \\\\(i\\\\) has a score higher or as high as all other scores, we make sure that between the score of the correct solution \\\\(\\y_i\\\\) and any other solution \\\\(\\y\\\\) is a margin at least as large at the loss \\\\(l(\\y,\\y_i)\\\\) incurred by predicting \\\\(\\y\\\\) instead of \\\\(\\y_i\\\\).\n$$\no(\\params) = \\sum_{(\\x_i,\\y_i) \\in \\mathcal{D}} s_{\\params}(\\x_i,\\y_i) - \\max_{\\y\\in\\mathcal{Y}} s_{\\params}(\\x_i,\\y) + l(\\y,\\y_i)     \n$$\n\nUsing a simple [Hamming Distance](https://en.wikipedia.org/wiki/Hamming_distance) loss between the labels of the prediction and the gold sequence, we can implement this model using the code below. We also use this opportunity to parametrize our functions by a generic model we can later on replace without redefining methods.  \n",
      "extraFields" : { },
      "outputFormat" : "<h4>Margin-Based Learning (Structured SVM)</h4><p>We mentioned that our training objective enforces no margin. This may lead to less robust models. To overcome this we can simply change the training objective to incorporate the notion of <a href=\"\">loss-augmented inference</a>. More concretely, instead of ony making sure that the correct solution <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-279\" role=\"math\" style=\"width: 1.015em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.551em 1000.003em 2.622em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-280\"><span class=\"msubsup\" id=\"MathJax-Span-281\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-282\"><span class=\"mrow\" id=\"MathJax-Span-283\"><span class=\"mi\" id=\"MathJax-Span-284\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 0.479em;\"><span class=\"mi\" id=\"MathJax-Span-285\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.004em; vertical-align: -0.354em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-21\">\\y_i</script> for training instance <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-286\" role=\"math\" style=\"width: 0.36em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.301em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.729em 1000.003em 2.741em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-287\"><span class=\"mi\" id=\"MathJax-Span-288\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.932em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-22\">i</script> has a score higher or as high as all other scores, we make sure that between the score of the correct solution <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-289\" role=\"math\" style=\"width: 1.015em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.551em 1000.003em 2.622em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-290\"><span class=\"msubsup\" id=\"MathJax-Span-291\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-292\"><span class=\"mrow\" id=\"MathJax-Span-293\"><span class=\"mi\" id=\"MathJax-Span-294\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 0.479em;\"><span class=\"mi\" id=\"MathJax-Span-295\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.004em; vertical-align: -0.354em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-23\">\\y_i</script> and any other solution <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-296\" role=\"math\" style=\"width: 0.658em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.539em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.908em 1000.003em 2.92em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-297\"><span class=\"texatom\" id=\"MathJax-Span-298\"><span class=\"mrow\" id=\"MathJax-Span-299\"><span class=\"mi\" id=\"MathJax-Span-300\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.932em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-24\">\\y</script> is a margin at least as large at the loss <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-301\" role=\"math\" style=\"width: 3.217em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.682em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.729em 1000.003em 2.979em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-302\"><span class=\"mi\" id=\"MathJax-Span-303\" style=\"font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-304\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-305\"><span class=\"mrow\" id=\"MathJax-Span-306\"><span class=\"mi\" id=\"MathJax-Span-307\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span class=\"mo\" id=\"MathJax-Span-308\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-309\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-310\"><span class=\"mrow\" id=\"MathJax-Span-311\"><span class=\"mi\" id=\"MathJax-Span-312\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 0.479em;\"><span class=\"mi\" id=\"MathJax-Span-313\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-314\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.289em; vertical-align: -0.354em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-25\">l(\\y,\\y_i)</script> incurred by predicting <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-315\" role=\"math\" style=\"width: 0.658em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.539em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.908em 1000.003em 2.92em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-316\"><span class=\"texatom\" id=\"MathJax-Span-317\"><span class=\"mrow\" id=\"MathJax-Span-318\"><span class=\"mi\" id=\"MathJax-Span-319\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.932em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-26\">\\y</script> instead of <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-320\" role=\"math\" style=\"width: 1.015em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.551em 1000.003em 2.622em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-321\"><span class=\"msubsup\" id=\"MathJax-Span-322\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-323\"><span class=\"mrow\" id=\"MathJax-Span-324\"><span class=\"mi\" id=\"MathJax-Span-325\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 0.479em;\"><span class=\"mi\" id=\"MathJax-Span-326\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.004em; vertical-align: -0.354em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-27\">\\y_i</script>. <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-327\" role=\"math\" style=\"width: 23.455em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 19.527em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.432em 1000.003em 4.17em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-328\"><span class=\"mi\" id=\"MathJax-Span-329\" style=\"font-family: STIXGeneral-Italic;\">o</span><span class=\"mo\" id=\"MathJax-Span-330\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-331\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span><span class=\"mo\" id=\"MathJax-Span-332\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-333\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.301em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-334\" style=\"padding-left: 0.301em;\"><span style=\"display: inline-block; position: relative; width: 2.801em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.86em 1000.003em 4.646em -999.997em); top: -3.985em; left: 0.777em;\"><span class=\"mo\" id=\"MathJax-Span-335\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.533em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.467em -999.997em); top: -2.854em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-336\"><span class=\"mrow\" id=\"MathJax-Span-337\"><span class=\"mo\" id=\"MathJax-Span-338\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-339\"><span style=\"display: inline-block; position: relative; width: 0.539em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.515em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-340\"><span class=\"mrow\" id=\"MathJax-Span-341\"><span class=\"mi\" id=\"MathJax-Span-342\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-weight: bold;\">x</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.866em; left: 0.36em;\"><span class=\"mi\" id=\"MathJax-Span-343\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-344\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-345\"><span style=\"display: inline-block; position: relative; width: 0.539em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.515em 1000.003em 4.289em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-346\"><span class=\"mrow\" id=\"MathJax-Span-347\"><span class=\"mi\" id=\"MathJax-Span-348\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.36em;\"><span class=\"mi\" id=\"MathJax-Span-349\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-350\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-351\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"texatom\" id=\"MathJax-Span-352\"><span class=\"mrow\" id=\"MathJax-Span-353\"><span class=\"mi\" id=\"MathJax-Span-354\" style=\"font-size: 70.7%; font-family: STIXNonUnicode-Italic;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-355\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-356\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"texatom\" id=\"MathJax-Span-357\"><span class=\"mrow\" id=\"MathJax-Span-358\"><span class=\"mi\" id=\"MathJax-Span-359\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-360\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-361\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-362\"><span class=\"mrow\" id=\"MathJax-Span-363\"><span class=\"mi\" id=\"MathJax-Span-364\" style=\"font-family: STIXGeneral; font-weight: bold;\">x</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.479em;\"><span class=\"mi\" id=\"MathJax-Span-365\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-366\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-367\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-368\"><span class=\"mrow\" id=\"MathJax-Span-369\"><span class=\"mi\" id=\"MathJax-Span-370\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 0.479em;\"><span class=\"mi\" id=\"MathJax-Span-371\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-372\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-373\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">−</span><span class=\"munderover\" id=\"MathJax-Span-374\" style=\"padding-left: 0.241em;\"><span style=\"display: inline-block; position: relative; width: 1.729em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mo\" id=\"MathJax-Span-375\" style=\"font-family: STIXGeneral-Regular;\">max</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.408em -999.997em); top: -3.271em; left: 0.182em;\"><span class=\"texatom\" id=\"MathJax-Span-376\"><span class=\"mrow\" id=\"MathJax-Span-377\"><span class=\"texatom\" id=\"MathJax-Span-378\"><span class=\"mrow\" id=\"MathJax-Span-379\"><span class=\"mi\" id=\"MathJax-Span-380\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span class=\"mo\" id=\"MathJax-Span-381\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"texatom\" id=\"MathJax-Span-382\"><span class=\"mrow\" id=\"MathJax-Span-383\"><span class=\"mi\" id=\"MathJax-Span-384\" style=\"font-size: 70.7%; font-family: STIXNonUnicode-Italic;\"><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.063em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-385\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-386\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"texatom\" id=\"MathJax-Span-387\"><span class=\"mrow\" id=\"MathJax-Span-388\"><span class=\"mi\" id=\"MathJax-Span-389\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-390\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-391\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-392\"><span class=\"mrow\" id=\"MathJax-Span-393\"><span class=\"mi\" id=\"MathJax-Span-394\" style=\"font-family: STIXGeneral; font-weight: bold;\">x</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.479em;\"><span class=\"mi\" id=\"MathJax-Span-395\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-396\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-397\" style=\"padding-left: 0.182em;\"><span class=\"mrow\" id=\"MathJax-Span-398\"><span class=\"mi\" id=\"MathJax-Span-399\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span class=\"mo\" id=\"MathJax-Span-400\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-401\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"mi\" id=\"MathJax-Span-402\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.241em;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-403\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-404\"><span class=\"mrow\" id=\"MathJax-Span-405\"><span class=\"mi\" id=\"MathJax-Span-406\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span class=\"mo\" id=\"MathJax-Span-407\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-408\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-409\"><span class=\"mrow\" id=\"MathJax-Span-410\"><span class=\"mi\" id=\"MathJax-Span-411\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 0.479em;\"><span class=\"mi\" id=\"MathJax-Span-412\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-413\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 3.004em; vertical-align: -1.782em;\"></span></span></nobr></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-28\"> o(\\params) = \\sum_{(\\x_i,\\y_i) \\in \\mathcal{D}} s_{\\params}(\\x_i,\\y_i) - \\max_{\\y\\in\\mathcal{Y}} s_{\\params}(\\x_i,\\y) + l(\\y,\\y_i) </script></p><p>Using a simple <a href=\"https://en.wikipedia.org/wiki/Hamming_distance\">Hamming Distance</a> loss between the labels of the prediction and the gold sequence, we can implement this model using the code below. We also use this opportunity to parametrize our functions by a generic model we can later on replace without redefining methods. </p>"
    }
  }, {
    "id" : 36,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "type Model = (X.Term, Y.Term) => DoubleTerm\n\ndef hamming(yGold:Y.Term, y:Y.Term) = \n  sum(0 until yGold.length) {i => 1.0 - I(yGold(i) === y(i))}   \n\ndef lossAugmented(m:Model)(x:X.Term, y:Y.Term, yGold:Y.Term) = \n  {m(x,y) + hamming(yGold,y)} argmaxBy maxProduct(BPParameters(1))\n  \ndef o(m:Model) = \n  sum(D){ xy => m(xy._1, xy._2) - \n                max(Y) {y => lossAugmented(m)(xy._1, y, xy._2)}} \n                \nval thetaStar = \n  { argmax(Thetas)(theta => o(model(theta))) by adaGrad(params) }.precalculate\n\ndef h(m:Model)(x:X.Term) = argmax(Y) {y => m(x,y)}\n\nval predictLinear = fun(X) {x => h(model(thetaStar))(x)}\n\n(predictLinear(test(0)._1) zip test(0)._1) take 4",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\",\"def o(theta:Thetas.Term) = \\n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}\",\"val params = AdaGradParameters(5, 0.1)\\nval thetaStar = argmax(Thetas)(theta => o(theta)) by adaGrad(params) \",\"val thetaStar = { argmax(Thetas)(theta => o(theta)) by adaGrad(params) }.precalculate\",\"def h(x:X.Term) = argmax(Y) {y => model(thetaStar)(x,y)}\",\"val yStar = h(X.Const(test(0)._1)).eval()\\n(yStar zip test(0)._1) take 4\",\"val predict = fun(X) {x => h(x)}\\n(predict(test(0)._1) zip test(0)._1) take 4\"]"
      },
      "outputFormat" : ""
    }
  } ],
  "config" : { }
}
