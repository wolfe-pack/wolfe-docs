{
  "name" : "Neural Linear Chain",
  "cells" : [ {
    "id" : 0,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import ml.wolfe.nlp.io.CoNLLReader\nval trainSents = new CoNLLReader(\"ml/wolfe/datasets/conll2000/train.txt\", \" \")\nval testSents = new CoNLLReader(\"ml/wolfe/datasets/conll2000/test.txt\", \" \")\ndef getChunkTags(s: Sentence) = {\n  val chunkTags = Array.fill(s.tokens.length)(\"O\")\n  s.ie.entityMentions.foreach { chunk =>\n    chunkTags(chunk.start) = if (chunk.label == \"O\") \"O\" else \"B-\" + chunk.label\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \"I-\" + chunk.label\n  }\n  collection.immutable.IndexedSeq(chunkTags: _*)\n}\n\ndef getPosTags(s: Sentence) = {\n  val posTags = s.tokens.map(_.posTag)\n  collection.immutable.IndexedSeq(posTags: _*)\n}\n\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\n  s.tokens.map(_.word.toString) -> getPosTags(s)\n}\n\nval train = trainSents.take(50).map(toInstance).toIndexedSeq\nval test = testSents.take(10).map(toInstance).toIndexedSeq\n1",
      "extraFields" : {
        "hide" : "true",
        "aggregatedCells" : "[]"
      },
      "outputFormat" : "<div class=\"string-result\"><span class=\"asString String\">1</span></div>"
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We like to predict the part-of-speech labels of tokens in text. We are given a `train` and `test` set, each with pairs of token sequences and tag sequences. Below we inspect the second training instance and print pairs of words and tags, for the first 5 tokens. ",
      "extraFields" : { },
      "outputFormat" : "<p>We like to predict the part-of-speech labels of tokens in text. We are given a <code>train</code> and <code>test</code> set, each with pairs of token sequences and tag sequences. Below we inspect the second training instance and print pairs of words and tags, for the first 5 tokens. </p>"
    }
  }, {
    "id" : 2,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "(train(1)._1.take(5) zip train(1)._2.take(5))",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(50).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n1\"]"
      },
      "outputFormat" : "<div class=\"string-result\"><div class=\"asIterable ArrayBuffer\"><span class=\"typeName\">ArrayBuffer</span>\n<ol start=\"0\" class=\"fields\">\n  <li class=\"fieldValue\"><div class=\"asProduct Tuple2\"><span class=\"typeName\">Tuple2</span>\n    <ul class=\"fields\">\n      <li class=\"field\"><span class=\"fieldName\">_1</span> <span class=\"fieldValue\"><span class=\"asString String\">Chancellor</span></span></li>\n      <li class=\"field\"><span class=\"fieldName\">_2</span> <span class=\"fieldValue\"><span class=\"asString String\">NNP</span></span></li>\n    </ul>\n</div></li>\n  <li class=\"fieldValue\"><div class=\"asProduct Tuple2\"><span class=\"typeName\">Tuple2</span>\n    <ul class=\"fields\">\n      <li class=\"field\"><span class=\"fieldName\">_1</span> <span class=\"fieldValue\"><span class=\"asString String\">of</span></span></li>\n      <li class=\"field\"><span class=\"fieldName\">_2</span> <span class=\"fieldValue\"><span class=\"asString String\">IN</span></span></li>\n    </ul>\n</div></li>\n  <li class=\"fieldValue\"><div class=\"asProduct Tuple2\"><span class=\"typeName\">Tuple2</span>\n    <ul class=\"fields\">\n      <li class=\"field\"><span class=\"fieldName\">_1</span> <span class=\"fieldValue\"><span class=\"asString String\">the</span></span></li>\n      <li class=\"field\"><span class=\"fieldName\">_2</span> <span class=\"fieldValue\"><span class=\"asString String\">DT</span></span></li>\n    </ul>\n</div></li>\n  <li class=\"fieldValue\"><div class=\"asProduct Tuple2\"><span class=\"typeName\">Tuple2</span>\n    <ul class=\"fields\">\n      <li class=\"field\"><span class=\"fieldName\">_1</span> <span class=\"fieldValue\"><span class=\"asString String\">Exchequer</span></span></li>\n      <li class=\"field\"><span class=\"fieldName\">_2</span> <span class=\"fieldValue\"><span class=\"asString String\">NNP</span></span></li>\n    </ul>\n</div></li>\n  <li class=\"fieldValue\"><div class=\"asProduct Tuple2\"><span class=\"typeName\">Tuple2</span>\n    <ul class=\"fields\">\n      <li class=\"field\"><span class=\"fieldName\">_1</span> <span class=\"fieldValue\"><span class=\"asString String\">Nigel</span></span></li>\n      <li class=\"field\"><span class=\"fieldName\">_2</span> <span class=\"fieldValue\"><span class=\"asString String\">NNP</span></span></li>\n    </ul>\n</div></li>\n</ol>\n</div></div>"
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "### A _Linear_ Linear Chain Model\nWe will first attempt to model this problem using a \"traditional\" first order linear chain model. These models score the sequence labels according to how well each individual label matches the sentence observations (for example, the word at the given token), and how consistent every two consecutive labels in the sequence are (for example, do determiners precede nouns). The scoring function extracts a feature representation for each single label, and for each consecutive pair of labels, and then multiplies these representations with a weight vector. \n\n\nMore formally, given a sequence of words \\\\(\\mathbf{x}\\\\), a sequence of labels \\\\(\\mathbf{y}\\\\), and a weight vector \\\\(\\boldsymbol{\\theta}\\\\), the model assigns the following score: \n$$\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\params}{\\boldsymbol{\\theta}}\n\\newcommand{\\feats}{\\boldsymbol{\\phi}}\ns_{\\params}(\\x,\\y) = \\sum_i^{|\\x|} \\langle \\params,\\feats_{\\text{local}}(\\x,y_i,i) \\rangle + \\sum_i^{|x|-1} \\langle \\params, \\feats_{\\text{transition}}(y_i,y_{i+1}) \\rangle \n$$\nThe model is linear both in the sense that the structure of the model is sequential, and that the scoring functions are linear with respect to th weights. \n\n#### Domains\nTo create this model in wolfe we first need to setup the [domains](/template/wolfe/wolfe-docs/concepts/02_terms) of values the model is concerned with: tags, words, and sequences of such values. Our first step is to collect the relevant values from the dataset. This includes upper bounds on the number of elements in each sequence.\n",
      "extraFields" : { },
      "outputFormat" : "<h3>A <em>Linear</em> Linear Chain Model</h3><p>We will first attempt to model this problem using a \"traditional\" first order linear chain model. These models score the sequence labels according to how well each individual label matches the sentence observations (for example, the word at the given token), and how consistent every two consecutive labels in the sequence are (for example, do determiners precede nouns). The scoring function extracts a feature representation for each single label, and for each consecutive pair of labels, and then multiplies these representations with a weight vector. </p><p>More formally, given a sequence of words <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-1\" role=\"math\" style=\"width: 0.658em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.539em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.908em 1000.003em 2.741em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"texatom\" id=\"MathJax-Span-3\"><span class=\"mrow\" id=\"MathJax-Span-4\"><span class=\"mi\" id=\"MathJax-Span-5\" style=\"font-family: STIXGeneral; font-weight: bold;\">x</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.718em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-1\">\\mathbf{x}</script>, a sequence of labels <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-6\" role=\"math\" style=\"width: 0.658em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.539em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.908em 1000.003em 2.92em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-7\"><span class=\"texatom\" id=\"MathJax-Span-8\"><span class=\"mrow\" id=\"MathJax-Span-9\"><span class=\"mi\" id=\"MathJax-Span-10\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.932em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-2\">\\mathbf{y}</script>, and a weight vector <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-11\" role=\"math\" style=\"width: 0.658em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.539em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.61em 1000.003em 2.682em -999.997em); top: -2.497em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-12\"><span class=\"mi\" id=\"MathJax-Span-13\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.004em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-3\">\\boldsymbol{\\theta}</script>, the model assigns the following score: <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-14\" role=\"math\" style=\"width: 28.336em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 23.574em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.598em 1000.003em 3.991em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-15\"><span class=\"msubsup\" id=\"MathJax-Span-16\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-17\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"texatom\" id=\"MathJax-Span-18\"><span class=\"mrow\" id=\"MathJax-Span-19\"><span class=\"mi\" id=\"MathJax-Span-20\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-21\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-22\"><span class=\"mrow\" id=\"MathJax-Span-23\"><span class=\"mi\" id=\"MathJax-Span-24\" style=\"font-family: STIXGeneral; font-weight: bold;\">x</span></span></span><span class=\"mo\" id=\"MathJax-Span-25\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-26\" style=\"padding-left: 0.182em;\"><span class=\"mrow\" id=\"MathJax-Span-27\"><span class=\"mi\" id=\"MathJax-Span-28\" style=\"font-family: STIXGeneral; font-weight: bold;\">y</span></span></span><span class=\"mo\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.301em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-31\" style=\"padding-left: 0.301em;\"><span style=\"display: inline-block; position: relative; width: 1.313em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.86em 1000.003em 4.646em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mo\" id=\"MathJax-Span-32\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.533em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.289em -999.997em); top: -2.854em; left: 0.539em;\"><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.217em 1000.003em 4.17em -999.997em); top: -5.176em; left: 0.36em;\"><span class=\"texatom\" id=\"MathJax-Span-34\"><span class=\"mrow\" id=\"MathJax-Span-35\"><span class=\"texatom\" id=\"MathJax-Span-36\"><span class=\"mrow\" id=\"MathJax-Span-37\"><span class=\"mo\" id=\"MathJax-Span-38\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">|</span></span></span><span class=\"texatom\" id=\"MathJax-Span-39\"><span class=\"mrow\" id=\"MathJax-Span-40\"><span class=\"mi\" id=\"MathJax-Span-41\" style=\"font-size: 70.7%; font-family: STIXGeneral; font-weight: bold;\">x</span></span></span><span class=\"texatom\" id=\"MathJax-Span-42\"><span class=\"mrow\" id=\"MathJax-Span-43\"><span class=\"mo\" id=\"MathJax-Span-44\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">|</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-45\" style=\"font-family: STIXGeneral-Regular;\">⟨</span><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span><span class=\"mo\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-48\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 2.146em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.098em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">ϕ</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.658em;\"><span class=\"texatom\" id=\"MathJax-Span-50\"><span class=\"mrow\" id=\"MathJax-Span-51\"><span class=\"mtext\" id=\"MathJax-Span-52\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">local</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-54\"><span class=\"mrow\" id=\"MathJax-Span-55\"><span class=\"mi\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral; font-weight: bold;\">x</span></span></span><span class=\"mo\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-58\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 0.717em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-62\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.182em;\">i</span><span class=\"mo\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Regular;\">⟩</span><span class=\"mo\" id=\"MathJax-Span-65\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.241em;\">+</span><span class=\"munderover\" id=\"MathJax-Span-66\" style=\"padding-left: 0.241em;\"><span style=\"display: inline-block; position: relative; width: 1.432em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.86em 1000.003em 4.646em -999.997em); top: -3.985em; left: 0.063em;\"><span class=\"mo\" id=\"MathJax-Span-67\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.533em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.336em 1000.003em 4.289em -999.997em); top: -2.854em; left: 0.598em;\"><span class=\"mi\" id=\"MathJax-Span-68\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; clip: rect(3.217em 1000.003em 4.17em -999.997em); top: -5.176em; left: 0.003em;\"><span class=\"texatom\" id=\"MathJax-Span-69\"><span class=\"mrow\" id=\"MathJax-Span-70\"><span class=\"texatom\" id=\"MathJax-Span-71\"><span class=\"mrow\" id=\"MathJax-Span-72\"><span class=\"mo\" id=\"MathJax-Span-73\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-74\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-75\"><span class=\"mrow\" id=\"MathJax-Span-76\"><span class=\"mo\" id=\"MathJax-Span-77\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">|</span></span></span><span class=\"mo\" id=\"MathJax-Span-78\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-79\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-80\" style=\"font-family: STIXGeneral-Regular;\">⟨</span><span class=\"mi\" id=\"MathJax-Span-81\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">θ</span><span class=\"mo\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-83\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 3.455em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.098em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-84\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">ϕ</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.658em;\"><span class=\"texatom\" id=\"MathJax-Span-85\"><span class=\"mrow\" id=\"MathJax-Span-86\"><span class=\"mtext\" id=\"MathJax-Span-87\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">transition</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-88\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-89\"><span style=\"display: inline-block; position: relative; width: 0.717em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-90\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"mi\" id=\"MathJax-Span-91\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-92\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-93\" style=\"padding-left: 0.182em;\"><span style=\"display: inline-block; position: relative; width: 1.551em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"texatom\" id=\"MathJax-Span-95\"><span class=\"mrow\" id=\"MathJax-Span-96\"><span class=\"mi\" id=\"MathJax-Span-97\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-98\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-99\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Regular;\">⟩</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 3.789em; vertical-align: -1.568em;\"></span></span></nobr></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-4\"> \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\y}{\\mathbf{y}} \\newcommand{\\params}{\\boldsymbol{\\theta}} \\newcommand{\\feats}{\\boldsymbol{\\phi}} s_{\\params}(\\x,\\y) = \\sum_i^{|\\x|} \\langle \\params,\\feats_{\\text{local}}(\\x,y_i,i) \\rangle + \\sum_i^{|x|-1} \\langle \\params, \\feats_{\\text{transition}}(y_i,y_{i+1}) \\rangle </script> The model is linear both in the sense that the structure of the model is sequential, and that the scoring functions are linear with respect to th weights. </p><h4>Domains</h4><p>To create this model in wolfe we first need to setup the <a href=\"/template/wolfe/wolfe-docs/concepts/02_terms\">domains</a> of values the model is concerned with: tags, words, and sequences of such values. Our first step is to collect the relevant values from the dataset. This includes upper bounds on the number of elements in each sequence.</p>"
    }
  }, {
    "id" : 4,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val words = (train ++ test).flatMap(_._1).distinct\nval tags = (train ++ test).flatMap(_._2).distinct\nval maxLength = (train ++ test).map(_._1.length).max",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(50).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n1\",\"(train(1)._1.take(5) zip train(1)._2.take(5))\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Notice that you don't need to get the values of domains in this way. Next you can create wolfe domain objects from these collections (notice we use upper case names for these by convention).",
      "extraFields" : { },
      "outputFormat" : "<p>Notice that you don't need to get the values of domains in this way. Next you can create wolfe domain objects from these collections (notice we use upper case names for these by convention).</p>"
    }
  }, {
    "id" : 6,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val Words = words.toDom \nval Tags = tags.toDom\nval Y = Seqs(Tags, 0, maxLength)\nval X = Seqs(Words, 0, maxLength)",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(50).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n1\",\"(train(1)._1.take(5) zip train(1)._2.take(5))\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 7,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Next we create the domain of parameter/feature vectors. For this we primarily need to choose a dimension. The easiest is to choose a number well above what we expect to be the number of parameters. If we overshoot this is not too bad.",
      "extraFields" : { },
      "outputFormat" : "<p>Next we create the domain of parameter/feature vectors. For this we primarily need to choose a dimension. The easiest is to choose a number well above what we expect to be the number of parameters. If we overshoot this is not too bad.</p>"
    }
  }, {
    "id" : 8,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val Thetas = Vectors(100000)",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(50).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n1\",\"(train(1)._1.take(5) zip train(1)._2.take(5))\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Feature Functions\nWe can now start defining the model by first specifying the feature functions \\\\(\\feats_{\\text{local}}\\\\) and \\\\(\\feats_{\\text{transition}}\\\\). We want these feature functions to return [sparse feature vectors](template/wolfe/wolfe-docs/concepts/05_sparse_feats), active at integer indices corresponding to certain features of the input. For this we need a feature index that maintains the mapping from features (such as pairs of tags and words) to integer indices.   \n\n",
      "extraFields" : { },
      "outputFormat" : "<h4>Feature Functions</h4><p>We can now start defining the model by first specifying the feature functions <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-102\" role=\"math\" style=\"width: 2.682em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.205em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.313em 1000.003em 2.562em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-103\"><span class=\"msubsup\" id=\"MathJax-Span-104\"><span style=\"display: inline-block; position: relative; width: 2.146em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.098em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-105\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">ϕ</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.658em;\"><span class=\"texatom\" id=\"MathJax-Span-106\"><span class=\"mrow\" id=\"MathJax-Span-107\"><span class=\"mtext\" id=\"MathJax-Span-108\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">local</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.218em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-5\">\\feats_{\\text{local}}</script> and <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-109\" role=\"math\" style=\"width: 4.229em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.515em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.313em 1000.003em 2.562em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-110\"><span class=\"msubsup\" id=\"MathJax-Span-111\"><span style=\"display: inline-block; position: relative; width: 3.455em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.098em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-family: STIXGeneral; font-style: italic; font-weight: bold;\">ϕ</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.658em;\"><span class=\"texatom\" id=\"MathJax-Span-113\"><span class=\"mrow\" id=\"MathJax-Span-114\"><span class=\"mtext\" id=\"MathJax-Span-115\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">transition</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.218em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-6\">\\feats_{\\text{transition}}</script>. We want these feature functions to return <a href=\"template/wolfe/wolfe-docs/concepts/05_sparse_feats\">sparse feature vectors</a>, active at integer indices corresponding to certain features of the input. For this we need a feature index that maintains the mapping from features (such as pairs of tags and words) to integer indices. </p>"
    }
  }, {
    "id" : 10,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val index = new SimpleFeatureIndex(Thetas)",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(50).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n1\",\"(train(1)._1.take(5) zip train(1)._2.take(5))\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 11,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "The local feature function will, for now, only consider a bias feature, and the conjunction of \\\\(x_i\\\\) and label \\\\(y_i\\\\) at a particular token index \\\\(i\\\\)",
      "extraFields" : { },
      "outputFormat" : "<p>The local feature function will, for now, only consider a bias feature, and the conjunction of <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-116\" role=\"math\" style=\"width: 0.955em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.61em 1000.003em 2.562em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-117\"><span class=\"msubsup\" id=\"MathJax-Span-118\"><span style=\"display: inline-block; position: relative; width: 0.717em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.17em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-119\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.861em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-7\">x_i</script> and label <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-121\" role=\"math\" style=\"width: 0.955em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.61em 1000.003em 2.562em -999.997em); top: -2.199em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-122\"><span class=\"msubsup\" id=\"MathJax-Span-123\"><span style=\"display: inline-block; position: relative; width: 0.717em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.396em 1000.003em 4.348em -999.997em); top: -3.985em; left: 0.003em;\"><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -3.807em; left: 0.42em;\"><span class=\"mi\" id=\"MathJax-Span-125\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.932em; vertical-align: -0.282em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-8\">y_i</script> at a particular token index <span class=\"MathJax_Preview\" style=\"color: inherit;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\"><nobr><span class=\"math\" id=\"MathJax-Span-126\" role=\"math\" style=\"width: 0.36em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.301em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.729em 1000.003em 2.741em -999.997em); top: -2.557em; left: 0.003em;\"><span class=\"mrow\" id=\"MathJax-Span-127\"><span class=\"mi\" id=\"MathJax-Span-128\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.562em;\"></span></span></span><span style=\"border-left-width: 0.004em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.932em; vertical-align: -0.068em;\"></span></span></nobr></span><script type=\"math/tex\" id=\"MathJax-Element-9\">i</script></p>"
    }
  }, {
    "id" : 12,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\n  index.oneHot('bias, y) + \n  index.oneHot('word, y, x(i))    \n}",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(50).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n1\",\"(train(1)._1.take(5) zip train(1)._2.take(5))\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 13,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We can test this feature function by instantiating it with some concrete (constant) terms and evaluating it to a vector. \n",
      "extraFields" : { },
      "outputFormat" : "<p>We can test this feature function by instantiating it with some concrete (constant) terms and evaluating it to a vector. </p>"
    }
  }, {
    "id" : 14,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val t = local(X.Const(IndexedSeq(\"the\",\"Chancellor\")), Tags.Const(\"NNP\"), 1)\nt.eval() //For an explicit map representation use index.toMap(t.eval())",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(50).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n1\",\"(train(1)._1.take(5) zip train(1)._2.take(5))\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\"]"
      },
      "outputFormat" : "<div class=\"string-result\"><span class=\"asString String\">SparseIndexedTensor npos=2 sorted=0 ind=10,6210,0,0 val=1.0,1.0,0.0,0.0</span></div>"
    }
  }, {
    "id" : 15,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We do the same thing for the transition features. ",
      "extraFields" : { },
      "outputFormat" : "<p>We do the same thing for the transition features. </p>"
    }
  }, {
    "id" : 16,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def transition(y1:Tags.Term, y2:Tags.Term) = {\n  index.oneHot('trans, y1, y2)    \n}",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(50).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n1\",\"(train(1)._1.take(5) zip train(1)._2.take(5))\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval()\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 17,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Linear Model\nWe are now ready to define the actual model, with an almost one-to-one correspondence to the math we wrote above. \n",
      "extraFields" : { },
      "outputFormat" : "<h4>Linear Model</h4><p>We are now ready to define the actual model, with an almost one-to-one correspondence to the math we wrote above. </p>"
    }
  }, {
    "id" : 18,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\n} ",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(50).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n1\",\"(train(1)._1.take(5) zip train(1)._2.take(5))\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval()\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\"]"
      },
      "outputFormat" : ""
    }
  }, {
    "id" : 19,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "The math misses one part of aspect of the model: the length of `y` should equal the length of `x`. Often such constraints are either kept implicit, or explictly stated in optimization problems that involve the model. We like to directly tie this constraint to the model. In addition we want to inform the learn and inference engine to use the Max-Product algorithm on this model whenever maximization is needed. This leads to the following definition.",
      "extraFields" : { },
      "outputFormat" : "<p>The math misses one part of aspect of the model: the length of <code>y</code> should equal the length of <code>x</code>. Often such constraints are either kept implicit, or explictly stated in optimization problems that involve the model. We like to directly tie this constraint to the model. In addition we want to inform the learn and inference engine to use the Max-Product algorithm on this model whenever maximization is needed. This leads to the following definition.</p>"
    }
  }, {
    "id" : 20,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \n  s(theta)(x,y) subjectTo (y.length === x.length) argmaxBy maxProduct(BPParameters(1))",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(50).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n1\",\"(train(1)._1.take(5) zip train(1)._2.take(5))\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval()\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} \"]"
      },
      "outputFormat" : "<div class=\"string-result\"><span class=\"asString String\">model: (theta: Thetas.Term)(x: X.Term, y: Y.Term)ml.wolfe.term.ProxyTerm[ml.wolfe.term.TypedDom[Double]]\n</span></div>"
    }
  } ],
  "config" : { }
}
