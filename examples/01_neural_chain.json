{
  "name" : "Linear Chains",
  "cells" : [ {
    "id" : 0,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "import ml.wolfe.nlp.io.CoNLLReader\nval trainSents = new CoNLLReader(\"ml/wolfe/datasets/conll2000/train.txt\", \" \")\nval testSents = new CoNLLReader(\"ml/wolfe/datasets/conll2000/test.txt\", \" \")\ndef getChunkTags(s: Sentence) = {\n  val chunkTags = Array.fill(s.tokens.length)(\"O\")\n  s.ie.entityMentions.foreach { chunk =>\n    chunkTags(chunk.start) = if (chunk.label == \"O\") \"O\" else \"B-\" + chunk.label\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \"I-\" + chunk.label\n  }\n  collection.immutable.IndexedSeq(chunkTags: _*)\n}\n\ndef getPosTags(s: Sentence) = {\n  val posTags = s.tokens.map(_.posTag)\n  collection.immutable.IndexedSeq(posTags: _*)\n}\n\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\n  s.tokens.map(_.word.toString) -> getPosTags(s)\n}\n\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\nval test = testSents.take(10).map(toInstance).toIndexedSeq\n\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\n1",
      "extraFields" : {
        "hide" : "true",
        "aggregatedCells" : "[]"
      }
    }
  }, {
    "id" : 1,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We like to predict the part-of-speech labels of tokens in text. We are given a `train` and `test` set, each with pairs of token sequences and tag sequences. \n\n#### Data\n\nBelow we inspect the second training instance and print pairs of words and tags, for the first 5 tokens. ",
      "extraFields" : { }
    }
  }, {
    "id" : 2,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "(train(1)._1 zip train(1)._2) take 5",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\"]"
      }
    }
  }, {
    "id" : 3,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "In the following we will introduce a few models to address this task, starting from standard linear chain structured prediction models with linear potentials to models with non-linear potentials (e.g., using a neural network within each potential).\n\n### A _Linear_ Linear Chain Model\nWe will first attempt to model this problem using a \"traditional\" first order linear chain model. These models score the sequence labels according to how well each individual label matches the sentence observations (for example, the word at the given token), and how consistent every two consecutive labels in the sequence are (for example, do determiners precede nouns). The scoring function extracts a feature representation for each single label, and for each consecutive pair of labels, and then multiplies these representations with a weight vector. \n\n\nMore formally, given a sequence of words \\\\(\\mathbf{x}\\\\), a sequence of labels \\\\(\\mathbf{y}\\\\), and a weight vector \\\\(\\boldsymbol{\\theta}\\\\), the model assigns the following score: \n$$\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\params}{\\boldsymbol{\\theta}}\n\\newcommand{\\feats}{\\boldsymbol{\\phi}}\ns_{\\params}(\\x,\\y) = \\sum_i^{|\\x|} \\langle \\params,\\feats_{\\text{local}}(\\x,y_i,i) \\rangle + \\sum_i^{|x|-1} \\langle \\params, \\feats_{\\text{transition}}(y_i,y_{i+1}) \\rangle \n$$\nThe model is linear both in the sense that the structure of the model is sequential, and that the scoring functions are linear with respect to the weights. \n\n#### Domains\nTo create this model in wolfe we first need to setup the [domains](/template/wolfe/wolfe-docs/concepts/02_terms) of values the model is concerned with: tags, words, and sequences of such values. Our first step is to collect the relevant values from the dataset. This includes upper bounds on the number of elements in each sequence.\n",
      "extraFields" : { }
    }
  }, {
    "id" : 4,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val words = (train ++ test).flatMap(_._1).distinct\nval tags = (train ++ test).flatMap(_._2).distinct\nval maxLength = (train ++ test).map(_._1.length).max",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\"]"
      }
    }
  }, {
    "id" : 5,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Notice that you don't need to get the values of domains in this way. Next you can create wolfe domain objects from these collections (notice we use upper case names for these by convention).",
      "extraFields" : { }
    }
  }, {
    "id" : 6,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val Words = words.toDom \nval Tags = tags.toDom\nval Y = Seqs(Tags, 0, maxLength)\nval X = Seqs(Words, 0, maxLength)",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\"]"
      }
    }
  }, {
    "id" : 7,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Next we create the domain of parameter/feature vectors. For this we primarily need to choose a dimension. The easiest is to choose a number well above what we expect to be the number of parameters. If we overshoot this is not too bad.",
      "extraFields" : { }
    }
  }, {
    "id" : 8,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val Thetas = Vectors(100000)",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\"]"
      }
    }
  }, {
    "id" : 9,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Feature Functions\nWe can now start defining the model by first specifying the feature functions \\\\(\\feats_{\\text{local}}\\\\) and \\\\(\\feats_{\\text{transition}}\\\\). We want these feature functions to return [sparse feature vectors](template/wolfe/wolfe-docs/concepts/05_sparse_feats), active at integer indices corresponding to certain features of the input. For this we need a feature index that maintains the mapping from features (such as pairs of tags and words) to integer indices.   \n\n",
      "extraFields" : { }
    }
  }, {
    "id" : 10,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val index = new SimpleFeatureIndex(Thetas)",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\"]"
      }
    }
  }, {
    "id" : 11,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "The local feature function will, for now, only consider a bias feature, and the conjunction of \\\\(x_i\\\\) and label \\\\(y_i\\\\) at a particular token index \\\\(i\\\\)",
      "extraFields" : { }
    }
  }, {
    "id" : 12,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\n  index.oneHot('bias, y) + \n  index.oneHot('word, y, x(i))    \n}",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\"]"
      }
    }
  }, {
    "id" : 13,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We can test this feature function by instantiating it with some concrete (constant) terms and evaluating it to a vector. \n",
      "extraFields" : { }
    }
  }, {
    "id" : 14,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val t = local(X.Const(IndexedSeq(\"the\",\"Chancellor\")), Tags.Const(\"NNP\"), 1)\nt.eval() //For an explicit map representation use index.toMap(t.eval())",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\"]"
      }
    }
  }, {
    "id" : 15,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We do the same thing for the transition features. ",
      "extraFields" : { }
    }
  }, {
    "id" : 16,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def transition(y1:Tags.Term, y2:Tags.Term) = {\n  index.oneHot('trans, y1, y2)    \n}",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\"]"
      }
    }
  }, {
    "id" : 17,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Linear Model\nWe are now ready to define the actual model, with an almost one-to-one correspondence to the math we wrote above. \n",
      "extraFields" : { }
    }
  }, {
    "id" : 18,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\n} subjectTo (y.length === x.length)",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\"]"
      }
    }
  }, {
    "id" : 19,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "Notice that thhe math missed one part of aspect of the model: the length of `y` should equal the length of `x`. Often such constraints are either kept implicit, or explictly stated in optimization problems that involve the model. We like to directly tie this constraint to the model, hence the `subjectTo` expression that makes sure that the model returns \\\\(-\\infty\\\\) if the length constraint is violated. \n\nIn addition we want to inform the learn and inference engine to use the Max-Product algorithm on this model whenever maximization is needed. This leads to the following definition.",
      "extraFields" : { }
    }
  }, {
    "id" : 20,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 21,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Training\nTraining the model means estimating good values for the parameter vector \\\\(\\params\\\\). In wolfe this can be achieved by defining a _learning objective_ (or negated training loss) and estimating its argmax. Our first learning objective will be a (negative) hinge-loss with zero margin. \n\nMathematically the zero-margin hinge-loss objective, for a given training set \\\\(\\mathcal{D}\\\\) is defined as follows:\n$$\no(\\params) = \\sum_{(\\x_i,\\y_i) \\in \\mathcal{D}} s_{\\params}(\\x_i,\\y_i) - \\max_{\\y\\in\\mathcal{Y}} s_{\\params}(\\x_i,\\y)    \n$$\nIntuitively, this objective encourages the score of the gold/true solution \\\\(\\y_i\\\\) to be the highest score. In this case the objective for instance \\\\(i\\\\) will be 0. You will note that this objective has a trivial solution (the zero parameter vector); however, running SGD (or variants) on it yields the perceptron algorithm, which will return a non-trivial solution.\n\nIn wolfe we can define this objective symbolically using terms. We can almost directly follow the math. As first step we need to convert our training set `train` into a term to represent \\\\(\\mathcal{D}\\\\). \n",
      "extraFields" : { }
    }
  }, {
    "id" : 22,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val D = Seqs(Pairs(X,Y),train.length).Const(train)",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\"]"
      }
    }
  }, {
    "id" : 23,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We can now define the function \\\\(o\\\\) in wolfe.",
      "extraFields" : { }
    }
  }, {
    "id" : 24,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def o(theta:Thetas.Term) = \n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}",
      "extraFields" : {
        "hide_output" : "true",
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\"]"
      }
    }
  }, {
    "id" : 25,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "We can now learn by finding a vector \\\\(\\params^*\\\\)that maximizes this objective. This can be done using the `argmax` operator in wolfe. ",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(50).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n1\",\"(train(1)._1.take(5) zip train(1)._2.take(5))\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} \",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) subjectTo (y.length === x.length) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\",\"def o(theta:Thetas.Term) = \\n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}\"]"
      }
    }
  }, {
    "id" : 26,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val params = AdaGradParameters(5, 0.1)\nval thetaStar = argmax(Thetas)(theta => o(theta)) by adaGrad(params) ",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\",\"def o(theta:Thetas.Term) = \\n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}\"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 27,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "At this point `thetaStar` is a term, a symbolic definition of the optimal weights, but no actual optimization has taken place. We could delay this further until the weights are needed for an actual `eval` call, but this may lead to long computations when we don't expect it. Instead we `precalculate` the value of this term now.  ",
      "extraFields" : { }
    }
  }, {
    "id" : 28,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val thetaStar = { argmax(Thetas)(theta => o(theta)) by adaGrad(params) }.precalculate",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\",\"def o(theta:Thetas.Term) = \\n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}\",\"val params = AdaGradParameters(5, 0.1)\\nval thetaStar = argmax(Thetas)(theta => o(theta)) by adaGrad(params) \"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 29,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Prediction\nWith the weight vector \\\\(\\params^*\\\\) we can now define the prediction function. This function is defined as\n$$\nh(\\x) = \\arg \\max_{\\y \\in Y} s_{\\params^*}(\\x,\\y)\n$$\nand can be formulated similarily in wolfe:",
      "extraFields" : { }
    }
  }, {
    "id" : 30,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "def h(x:X.Term) = argmax(Y) {y => model(thetaStar)(x,y)}",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\",\"def o(theta:Thetas.Term) = \\n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}\",\"val params = AdaGradParameters(5, 0.1)\\nval thetaStar = argmax(Thetas)(theta => o(theta)) by adaGrad(params) \",\"val thetaStar = { argmax(Thetas)(theta => o(theta)) by adaGrad(params) }.precalculate\"]",
        "hide_output" : "true"
      }
    }
  }, {
    "id" : 31,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "To apply the predictor to an inpux _value_ we need to convert the value to a term (constant), apply the predictor function to this constant, and then evaluate the term.",
      "extraFields" : { }
    }
  }, {
    "id" : 32,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val yStar = h(X.Const(test(0)._1)).eval()\n(yStar zip test(0)._1) take 4",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\",\"def o(theta:Thetas.Term) = \\n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}\",\"val params = AdaGradParameters(5, 0.1)\\nval thetaStar = argmax(Thetas)(theta => o(theta)) by adaGrad(params) \",\"val thetaStar = { argmax(Thetas)(theta => o(theta)) by adaGrad(params) }.precalculate\",\"def h(x:X.Term) = argmax(Y) {y => model(thetaStar)(x,y)}\"]"
      }
    }
  }, {
    "id" : 33,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "In practice you will often want to use a trained model in the above fashion: some input _value_ is observed, and some prediction is made based on this observation. Doing this repeatedly as shown above can be quite inefficient, as for each prediction a new `eval` call is necessary, which requires memory allocation, term tree optimizations etc. To simplify this use case, wolfe provides the `fun` method which can create efficient functions that map values to the results of some term tree. ",
      "extraFields" : { }
    }
  }, {
    "id" : 34,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "val predict = fun(X) {x => h(x)}\n(predict(test(0)._1) zip test(0)._1) take 4",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\",\"def o(theta:Thetas.Term) = \\n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}\",\"val params = AdaGradParameters(5, 0.1)\\nval thetaStar = argmax(Thetas)(theta => o(theta)) by adaGrad(params) \",\"val thetaStar = { argmax(Thetas)(theta => o(theta)) by adaGrad(params) }.precalculate\",\"def h(x:X.Term) = argmax(Y) {y => model(thetaStar)(x,y)}\",\"val yStar = h(X.Const(test(0)._1)).eval()\\n(yStar zip test(0)._1) take 4\"]"
      }
    }
  }, {
    "id" : 35,
    "compiler" : "markdown",
    "input" : {
      "sessionId" : null,
      "code" : "#### Margin-Based Learning (Structured SVM)\nWe mentioned that our training objective enforces no margin. This may lead to less robust models. To overcome this we can simply change the training objective to incorporate the notion of [loss-augmented inference](). More concretely, instead of ony making sure that the correct solution \\\\(\\y_i\\\\) for training instance \\\\(i\\\\) has a score higher or as high as all other scores, we make sure that between the score of the correct solution \\\\(\\y_i\\\\) and any other solution \\\\(\\y\\\\) is a margin at least as large at the loss \\\\(l(\\y,\\y_i)\\\\) incurred by predicting \\\\(\\y\\\\) instead of \\\\(\\y_i\\\\).\n$$\no(\\params) = \\sum_{(\\x_i,\\y_i) \\in \\mathcal{D}} s_{\\params}(\\x_i,\\y_i) - \\max_{\\y\\in\\mathcal{Y}} s_{\\params}(\\x_i,\\y) + l(\\y,\\y_i)     \n$$\n\nUsing a simple [Hamming Distance](https://en.wikipedia.org/wiki/Hamming_distance) loss between the labels of the prediction and the gold sequence, we can implement this model using the code below. We also use this opportunity to parametrize our functions by a generic model we can later on replace without redefining methods.  \n",
      "extraFields" : { }
    }
  }, {
    "id" : 36,
    "compiler" : "scala",
    "input" : {
      "sessionId" : null,
      "code" : "type Model = (X.Term, Y.Term) => DoubleTerm\n\ndef hamming(yGold:Y.Term, y:Y.Term) = \n  sum(0 until yGold.length) {i => 1.0 - I(yGold(i) === y(i))}   \n\ndef lossAugmented(m:Model)(x:X.Term, y:Y.Term, yGold:Y.Term) = \n  {m(x,y) + hamming(yGold,y)} argmaxBy maxProduct(BPParameters(1))\n  \ndef o(m:Model) = \n  sum(D){ xy => m(xy._1, xy._2) - \n                max(Y) {y => lossAugmented(m)(xy._1, y, xy._2)}} \n                \nval thetaStar = \n  { argmax(Thetas)(theta => o(model(theta))) by adaGrad(params) }.precalculate\n\ndef h(m:Model)(x:X.Term) = argmax(Y) {y => m(x,y)}\n\nval predictLinear = fun(X) {x => h(model(thetaStar))(x)}\n\n(predictLinear(test(0)._1) zip test(0)._1) take 4",
      "extraFields" : {
        "aggregatedCells" : "[\"import ml.wolfe.nlp.io.CoNLLReader\\nval trainSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/train.txt\\\", \\\" \\\")\\nval testSents = new CoNLLReader(\\\"ml/wolfe/datasets/conll2000/test.txt\\\", \\\" \\\")\\ndef getChunkTags(s: Sentence) = {\\n  val chunkTags = Array.fill(s.tokens.length)(\\\"O\\\")\\n  s.ie.entityMentions.foreach { chunk =>\\n    chunkTags(chunk.start) = if (chunk.label == \\\"O\\\") \\\"O\\\" else \\\"B-\\\" + chunk.label\\n    for (i <- chunk.start + 1 until chunk.end) chunkTags(i) = \\\"I-\\\" + chunk.label\\n  }\\n  collection.immutable.IndexedSeq(chunkTags: _*)\\n}\\n\\ndef getPosTags(s: Sentence) = {\\n  val posTags = s.tokens.map(_.posTag)\\n  collection.immutable.IndexedSeq(posTags: _*)\\n}\\n\\ndef toInstance(s: Sentence): (IndexedSeq[String], IndexedSeq[String]) = {\\n  //s.tokens.map(_.word.toString) -> getChunkTags(s)\\n  s.tokens.map(_.word.toString) -> getPosTags(s)\\n}\\n\\nval train = trainSents.take(10).map(toInstance).toIndexedSeq\\nval test = testSents.take(10).map(toInstance).toIndexedSeq\\n\\ndef precalculate[T<:Term[Dom]](t:T) = t.precalculate\\n1\",\"(train(1)._1 zip train(1)._2) take 5\",\"val words = (train ++ test).flatMap(_._1).distinct\\nval tags = (train ++ test).flatMap(_._2).distinct\\nval maxLength = (train ++ test).map(_._1.length).max\",\"val Words = words.toDom \\nval Tags = tags.toDom\\nval Y = Seqs(Tags, 0, maxLength)\\nval X = Seqs(Words, 0, maxLength)\",\"val Thetas = Vectors(100000)\",\"val index = new SimpleFeatureIndex(Thetas)\",\"def local(x:X.Term, y:Tags.Term, i:IntTerm) = {\\n  index.oneHot('bias, y) + \\n  index.oneHot('word, y, x(i))    \\n}\",\"val t = local(X.Const(IndexedSeq(\\\"the\\\",\\\"Chancellor\\\")), Tags.Const(\\\"NNP\\\"), 1)\\nt.eval() //For an explicit map representation use index.toMap(t.eval())\",\"def transition(y1:Tags.Term, y2:Tags.Term) = {\\n  index.oneHot('trans, y1, y2)    \\n}\",\"def s(theta:Thetas.Term)(x:X.Term, y:Y.Term) = {\\n  sum(0 until x.length) {i => theta dot local(x, y(i), i)} + \\n  sum(0 until x.length - 1) {i => theta dot transition(y(i), y(i+1))}\\n} subjectTo (y.length === x.length)\",\"def model(theta:Thetas.Term)(x:X.Term, y:Y.Term) = \\n  s(theta)(x,y) argmaxBy maxProduct(BPParameters(1))\",\"val D = Seqs(Pairs(X,Y),train.length).Const(train)\",\"def o(theta:Thetas.Term) = \\n  sum(D){ xy => model(theta)(xy._1,xy._2) - max(Y) {y => model(theta)(xy._1,y)}}\",\"val params = AdaGradParameters(5, 0.1)\\nval thetaStar = argmax(Thetas)(theta => o(theta)) by adaGrad(params) \",\"val thetaStar = { argmax(Thetas)(theta => o(theta)) by adaGrad(params) }.precalculate\",\"def h(x:X.Term) = argmax(Y) {y => model(thetaStar)(x,y)}\",\"val yStar = h(X.Const(test(0)._1)).eval()\\n(yStar zip test(0)._1) take 4\",\"val predict = fun(X) {x => h(x)}\\n(predict(test(0)._1) zip test(0)._1) take 4\"]"
      }
    }
  } ],
  "config" : { }
}
