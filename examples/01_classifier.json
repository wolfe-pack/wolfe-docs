{
  "name" : "Classifiers",
  "cells" : [ {
    "id" : 0,
    "compiler" : "markdown",
    "input" : {
      "code" : "Supervised classification is an important task in many real-world applications. In Wolfe \nclassification amounts to fixing the values of attributes possible worlds such that only\none atomic attribute remains unconstrained. ",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 1,
    "compiler" : "heading3",
    "input" : {
      "code" : "Logical Gate",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 2,
    "compiler" : "markdown",
    "input" : {
      "code" : "In this example we learn a binary [AND gate](http://en.wikipedia.org/wiki/AND_gate) from data.\nFirst we define the data structure to define a distribution over. \nThe case class `XY` has three fields, two (`in1` and `in2`) for the input values to the gate,\nand one (`out`) for the output of the gate. The `andData` sequence stores \nthe input/output behavior of the gate.\n\n",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 3,
    "compiler" : "wolfe",
    "input" : {
      "code" : "case class XY(in1: Boolean, in2: Boolean, out: Boolean)\nval andData = Seq(\n  XY(true, true, true),\n  XY(false, true, false),\n  XY(true, false, false),\n  XY(false, false, false))\nandData.head.in1 && andData.head.in2 == andData.head.out",
      "outputFormat" : "string",
      "extraFields" : null
    }
  }, {
    "id" : 4,
    "compiler" : "markdown",
    "input" : {
      "code" : "The model `s_gate` is a classifier with 3 features,\none for the conjunction of input 1 and output, one for the conjunction of input 2\nand output, and a bias feature. We implement this model as the dot-product\nof a feature vector `f_gate` and a weight vector.",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 5,
    "compiler" : "wolfe",
    "input" : {
      "code" : "def f_gate(d: XY) = \n  oneHot('in1, I(d.in1 && d.out)) + \n  oneHot('in2, I(d.in2 && d.out)) + \n  oneHot('bias, I(d.out))\n\ndef s_gate(w: Vector)(d: XY) = w dot f_gate(d)\n\nf_gate(andData.head)",
      "outputFormat" : "string",
      "extraFields" : null
    }
  }, {
    "id" : 6,
    "compiler" : "markdown",
    "input" : {
      "code" : "Next we define the predictor `h_gate` that takes as input a weight vector `w` and \nan observation `obs`. It finds the `XY` value consistent with the observation \nand with maximal score. An `XY` value `d` is consistent with an observation `obs`\nwhen their input fields match. This is captured using `q_gate`. \n\nThe weight vector `w_opt` is one optimal solution. That is, it would classify\nall instances correctly.  ",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 7,
    "compiler" : "wolfe",
    "input" : {
      "code" : "def q_gate(obs: XY)(d: XY) = d.in1 == obs.in1 && d.in2 == obs.in2\n\ndef h_gate(w: Vector)(obs: XY) = \n  argmax(all(XY) where q_gate(obs)) { s_gate(w) }\n\nval w_opt = Vector('bias -> -3.0, 'in1 -> 2.0, 'in2 -> 1.5)  \n\nandData forall (d => h_gate(w_opt)(d).out == d.out)",
      "outputFormat" : "string",
      "extraFields" : null
    }
  }, {
    "id" : 8,
    "compiler" : "markdown",
    "input" : {
      "code" : "Now instead of manually defining the weight vector, we will learn it from data. To\nthis end we define a training loss `l_gate` which encourages the predictions\n`h_gate(w)(d)` to be the training data `d`, as this will make the\nterm `s_gate(w)(h_gate(w)(d)) - s_gate(w)(d)` become zero. Minimizing this loss\nusing subgradient methods yields the perceptron algorithm. However, we note \nthat this loss always also has the 0 vector as solution, and for non-separable data\nthis in fact would be the optimum. \n\nNote how the training loss function `l_gate` is annotated. This annotation\ntells wolfe how to minimize this function. In this case wolfe is told to use\nan online learner/optimizer and to use 10 training epochs. This optimizer\nis used to find the argmin `w_gate`. The evaluator shows that we have indeed\nlearned the gate function. \n",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 9,
    "compiler" : "wolfe",
    "input" : {
      "code" : "import cc.factorie.optimize.{Perceptron, OnlineTrainer}\nimport ml.wolfe.util.Evaluator\n\n@OptimizeByLearning(Learn.online(10))\ndef l_gate(data: Seq[XY])(w: Vector) = \n  sum(data) { d => s_gate(w)(h_gate(w)(d)) - s_gate(w)(d) }\n\nval w_gate = argmin(vectors) { l_gate(andData) }\nEvaluator.evaluate(andData, andData.map(h_gate(w_gate)))(_.out)",
      "outputFormat" : "string",
      "extraFields" : null
    }
  }, {
    "id" : 10,
    "compiler" : "heading3",
    "input" : {
      "code" : "Iris Dataset",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 11,
    "compiler" : "markdown",
    "input" : {
      "code" : "The [Iris data set](http://archive.ics.uci.edu/ml/datasets/Iris) \ncontains 3 classes of 50 instances each, where each class refers to a type of iris plant.\nBelow you see the definition of the data structures needed. Note that \nthe observed attributes are `Double` values and that Wolfe currently\nrequires such attributes to be observed. Also note that in the snippets following\nthe one below we use predefined data structures; the ones introduced here \nonly serve illustration purposes.",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 12,
    "compiler" : "wolfe",
    "input" : {
      "code" : "object Data {\n  case class Label(label: String)\n  case class IrisData(sepalLength: Double, \n                      sepalWidth: Double, \n                      petalLength: Double, \n                      petalWidth: Double, \n                      irisClass: Label)\n  implicit val classes = Seq(\n    Label(\"Iris-setosa\"), \n    Label(\"Iris-versicolor\"), \n    Label(\"Iris-virginica\"))\n}",
      "outputFormat" : "string",
      "extraFields" : null
    }
  }, {
    "id" : 13,
    "compiler" : "markdown",
    "input" : {
      "code" : "We first load the data, shuffle it and then split it equally into a training and test set. This\nis all standard Scala code, and not specific to wolfe. ",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 14,
    "compiler" : "wolfe",
    "input" : {
      "code" : "import ml.wolfe.util.Iris._\nval random = new scala.util.Random(0)\nval dataset = random.shuffle(loadIris())\nval (train,test) = dataset.splitAt(dataset.size / 2)\ntrain.head",
      "outputFormat" : "string",
      "extraFields" : null
    }
  }, {
    "id" : 15,
    "compiler" : "markdown",
    "input" : {
      "code" : "In the model we have one feature per attribute of the flower. The feature function\n`f_iris` constructs a feature vector of this kind. Notice how feature representations\nare defined in terms of both input and output fields: each oneHot vector is indexed\nat a position corresponding to the value of the output class, and the hot value\ncorresponds to the real value associated with the input field. ",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 16,
    "compiler" : "wolfe",
    "input" : {
      "code" : "def f_iris(d:IrisData) = \n  oneHot('sl -> d.irisClass, d.sepalLength) +\n  oneHot('sw -> d.irisClass, d.sepalWidth) +\n  oneHot('pl -> d.irisClass, d.petalLength) +\n  oneHot('pw -> d.irisClass, d.petalWidth)\nf_iris(train.head)",
      "outputFormat" : "string",
      "extraFields" : null
    }
  }, {
    "id" : 17,
    "compiler" : "markdown",
    "input" : {
      "code" : "We define the model again as the dot product of feature vector \nand weight vector. Next we define `obs_iris` as the observation\ncondition. Notice how we condition on everything but the class label. More details on \nthis notation can be found in the section on \n[constraints](/doc/wolfe-static/wolfe/docs/concepts/02_constraints).\n\nThe predictor `h_iris` takes a weight vector `w` and an observation `i` to \nreturn the `IrisData` object with highest score and consistent with `i`, where\nconsistency is defined through `obs_iris`. \n\nTo learn we again define a perceptron-style objective `l_iris` and then minimize\nit to get `w_iris`. Then we evaluate this weight vector on the test data. \n\n",
      "outputFormat" : "html",
      "extraFields" : null
    }
  }, {
    "id" : 18,
    "compiler" : "wolfe",
    "input" : {
      "code" : "def s_iris(w:Vector)(d:IrisData) = f_iris(d) dot w\ndef obs_iris(d:IrisData) = d.copy(irisClass = hidden)\ndef worlds = all(IrisData)\ndef h_iris(w:Vector)(i:IrisData) = \n  argmax(worlds where (obs_iris(_) == obs_iris(i))) { s_iris(w) }\n\ndef l_iris(data:Seq[IrisData])(w:Vector) =\n  sum(data) {i => s_iris(w)(h_iris(w)(i)) - s_iris(w)(i)}\n\nval w_iris = argmin(vectors)(l_iris(train))\nEvaluator.evaluate(test, test map(h_iris(w_iris)))(_.irisClass)",
      "outputFormat" : "string",
      "extraFields" : null
    }
  } ]
}
